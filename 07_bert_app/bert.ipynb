{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# BERT\n",
    "\n",
    "In this notebook, you will understand how to use Transformers introduced in [Vaswani et al., 2017]  You will learn how to load a pretrained Transformer model and evaluate it on `newstest2014`. In addition, you are able to translate a few sentences youself with the `BeamSearchTranslator`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Preparation\n",
    "\n",
    "We start with some usual preparation such as importing libraries and setting the environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, nd\n",
    "import gluonnlp as nlp\n",
    "\n",
    "np.random.seed(100)\n",
    "random.seed(100)\n",
    "mx.random.seed(10000)\n",
    "ctx = mx.gpu(0)\n",
    "\n",
    "from utils import PositionalEncoding, MultiHeadAttention, AddNorm, PositionWiseFFN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The position-wise feed-forward network accepts a 3-dim input with shape (batch size, sequence length, feature size). It consists of two dense layers that applies to the last dimension, which means the same dense layers are used for each position item in the sequence, so called position-wise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Now we define the transformer block for the encoder, which contains a multi-head attention layer, a position-wise feed-forward network, and two connection blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "12"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class EncoderBlock(gluon.nn.Block):\n",
    "    def __init__(self, units, hidden_size, num_heads, dropout, **kwargs):\n",
    "        super(EncoderBlock, self).__init__(**kwargs)\n",
    "        self.attention = MultiHeadAttention(units, num_heads, dropout)\n",
    "        self.add_1 = AddNorm(dropout)\n",
    "        self.ffn = PositionWiseFFN(units, hidden_size)\n",
    "        self.add_2 = AddNorm(dropout)\n",
    "\n",
    "    def forward(self, X, mask):\n",
    "        Y = self.add_1(X, self.attention(X, X, X, mask))\n",
    "        return self.add_2(Y, self.ffn(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Due to the residual connections, this block will not change the input shape. It means the `units` argument should be equal to the input's last dimension size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "13"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 100, 24)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_blk = EncoderBlock(24, 48, 8, 0.5)\n",
    "encoder_blk.initialize()\n",
    "mask = nd.ones(shape=(2, 100, 100))\n",
    "mask[0, :, 2:] = 0\n",
    "mask[1, :, 3:] = 0\n",
    "encoder_blk(nd.ones((2, 100, 24)), mask).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The encoder stacks $n$ blocks. Due to the residual connection again, the embedding layer size $d$ is same as the transformer block output size. Also note that we multiple the embedding output by $\\sqrt{d}$ to avoid its values are too small compared to positional encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "14"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(gluon.nn.Block):\n",
    "    def __init__(self, vocab_size, units, hidden_size,\n",
    "                 num_heads, num_layers, dropout, **kwargs):\n",
    "        super(TransformerEncoder, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.embed = gluon.nn.Embedding(vocab_size, units)\n",
    "        self.pos_encoding = PositionalEncoding(units, dropout)\n",
    "        self.blks = gluon.nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.blks.add(\n",
    "                EncoderBlock(units, hidden_size, num_heads, dropout))\n",
    "\n",
    "    def forward(self, X, mask, *args):\n",
    "        X = self.pos_encoding(self.embed(X) * math.sqrt(self.units))\n",
    "        for blk in self.blks:\n",
    "            X = blk(X, mask)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Create an encoder with two transformer blocks, whose hyper-parameters are same as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "15"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 100, 24)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = TransformerEncoder(200, 24, 48, 8, 2, 0.5)\n",
    "encoder.initialize()\n",
    "encoder(nd.ones((2, 100)), mask).shape"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
