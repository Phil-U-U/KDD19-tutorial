{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine Translation with Transformer\n",
    "\n",
    "In this notebook, you will understand how to use Transformers introduced in [Vaswani et al., 2017]  You will learn how to load a pretrained Transformer model and evaluate it on `newstest2014`. In addition, you are able to translate a few sentences youself with the `BeamSearchTranslator`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Preparation\n",
    "\n",
    "We start with some usual preparation such as importing libraries and setting the environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T22:51:39.225344Z",
     "start_time": "2019-07-25T22:51:37.299459Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.highlight .hll { background-color: #ffffcc }\n",
       ".highlight  { background: #f8f8f8; }\n",
       ".highlight .c { color: #408080; font-style: italic } /* Comment */\n",
       ".highlight .err { border: 1px solid #FF0000 } /* Error */\n",
       ".highlight .k { color: #008000; font-weight: bold } /* Keyword */\n",
       ".highlight .o { color: #666666 } /* Operator */\n",
       ".highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */\n",
       ".highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */\n",
       ".highlight .cp { color: #BC7A00 } /* Comment.Preproc */\n",
       ".highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */\n",
       ".highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */\n",
       ".highlight .cs { color: #408080; font-style: italic } /* Comment.Special */\n",
       ".highlight .gd { color: #A00000 } /* Generic.Deleted */\n",
       ".highlight .ge { font-style: italic } /* Generic.Emph */\n",
       ".highlight .gr { color: #FF0000 } /* Generic.Error */\n",
       ".highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n",
       ".highlight .gi { color: #00A000 } /* Generic.Inserted */\n",
       ".highlight .go { color: #888888 } /* Generic.Output */\n",
       ".highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n",
       ".highlight .gs { font-weight: bold } /* Generic.Strong */\n",
       ".highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n",
       ".highlight .gt { color: #0044DD } /* Generic.Traceback */\n",
       ".highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n",
       ".highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n",
       ".highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n",
       ".highlight .kp { color: #008000 } /* Keyword.Pseudo */\n",
       ".highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n",
       ".highlight .kt { color: #B00040 } /* Keyword.Type */\n",
       ".highlight .m { color: #666666 } /* Literal.Number */\n",
       ".highlight .s { color: #BA2121 } /* Literal.String */\n",
       ".highlight .na { color: #7D9029 } /* Name.Attribute */\n",
       ".highlight .nb { color: #008000 } /* Name.Builtin */\n",
       ".highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */\n",
       ".highlight .no { color: #880000 } /* Name.Constant */\n",
       ".highlight .nd { color: #AA22FF } /* Name.Decorator */\n",
       ".highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */\n",
       ".highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */\n",
       ".highlight .nf { color: #0000FF } /* Name.Function */\n",
       ".highlight .nl { color: #A0A000 } /* Name.Label */\n",
       ".highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */\n",
       ".highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */\n",
       ".highlight .nv { color: #19177C } /* Name.Variable */\n",
       ".highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */\n",
       ".highlight .w { color: #bbbbbb } /* Text.Whitespace */\n",
       ".highlight .mb { color: #666666 } /* Literal.Number.Bin */\n",
       ".highlight .mf { color: #666666 } /* Literal.Number.Float */\n",
       ".highlight .mh { color: #666666 } /* Literal.Number.Hex */\n",
       ".highlight .mi { color: #666666 } /* Literal.Number.Integer */\n",
       ".highlight .mo { color: #666666 } /* Literal.Number.Oct */\n",
       ".highlight .sa { color: #BA2121 } /* Literal.String.Affix */\n",
       ".highlight .sb { color: #BA2121 } /* Literal.String.Backtick */\n",
       ".highlight .sc { color: #BA2121 } /* Literal.String.Char */\n",
       ".highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */\n",
       ".highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n",
       ".highlight .s2 { color: #BA2121 } /* Literal.String.Double */\n",
       ".highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */\n",
       ".highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */\n",
       ".highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */\n",
       ".highlight .sx { color: #008000 } /* Literal.String.Other */\n",
       ".highlight .sr { color: #BB6688 } /* Literal.String.Regex */\n",
       ".highlight .s1 { color: #BA2121 } /* Literal.String.Single */\n",
       ".highlight .ss { color: #19177C } /* Literal.String.Symbol */\n",
       ".highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */\n",
       ".highlight .fm { color: #0000FF } /* Name.Function.Magic */\n",
       ".highlight .vc { color: #19177C } /* Name.Variable.Class */\n",
       ".highlight .vg { color: #19177C } /* Name.Variable.Global */\n",
       ".highlight .vi { color: #19177C } /* Name.Variable.Instance */\n",
       ".highlight .vm { color: #19177C } /* Name.Variable.Magic */\n",
       ".highlight .il { color: #666666 } /* Literal.Number.Integer.Long */</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, nd\n",
    "from mxnet.gluon import nn\n",
    "import gluonnlp as nlp\n",
    "\n",
    "np.random.seed(100)\n",
    "random.seed(100)\n",
    "mx.random.seed(10000)\n",
    "ctx = mx.gpu(0)\n",
    "\n",
    "from utils import setup_source, source\n",
    "setup_source()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The Transformer model is also based on the encoder-decoder architecture. It,\n",
    "however, differs to the seq2seq model that the transformer replaces the\n",
    "recurrent layers in seq2seq with attention layers. To deal with sequential\n",
    "inputs, each item in the sequential is copied as the query, the key and the\n",
    "value as illustrated in :numref:`fig_self_attention`. It therefore outputs a same length\n",
    "sequential output. We call such an attention layer as a self-attention layer.\n",
    "\n",
    "![Self-attention architecture.](../img/self-attention.svg)\n",
    "\n",
    ":label:`fig_self_attention`\n",
    "\n",
    "\n",
    "\n",
    "<!-- Compared to a recurrent layer, output items of a self-attention layer can be computed in parallel and, therefore, it is easy to obtain a high-efficient implementation. -->\n",
    "\n",
    "The transformer architecture, with a comparison to the seq2seq model with\n",
    "attention, is shown in :numref:`fig_transformer`. These two models are similar to\n",
    "each other in overall: the source sequence embeddings are fed into $n$ repeated\n",
    "blocks. The outputs of the last block are then used as attention memory for the\n",
    "decoder.  The target sequence embeddings is similarly fed into $n$ repeated\n",
    "blocks in the decoder, and the final outputs are obtained by applying a dense\n",
    "layer with vocabulary size to the last block's outputs.\n",
    "\n",
    "![The transformer architecture.](../img/transformer.svg)\n",
    "\n",
    ":label:`fig_transformer`\n",
    "\n",
    "\n",
    "It can also be seen that the transformer differs to the seq2seq with attention model in three major places:\n",
    "\n",
    "1. A recurrent layer in seq2seq is replaced with a transformer block. This block contains a self-attention layer (multi-head attention) and a network with two dense layers (position-wise FFN) for the encoder. For the decoder, another multi-head attention layer is used to take the encoder state.\n",
    "1. The encoder state is passed to every transformer block in the decoder, instead of using as an additional input of the first recurrent layer in seq2seq.\n",
    "1. Since the self-attention layer does not distinguish the item order in a sequence, a positional encoding layer is used to add sequential information into each sequence item.\n",
    "\n",
    "In the rest of this section, we will explain every new layer introduced by the transformer, and construct a model to train on the machine translation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Mechanism\n",
    "\n",
    "In :numref:`chapter_seq2seq`, we encode the source sequence input information in the recurrent unit state and then pass it to the decoder to generate the target sequence. A token in the target sequence may closely relate to some tokens in the source sequence instead of the whole source sequence. For example, when translating \"Hello world.\" to \"Bonjour le monde.\", \"Bonjour\" maps to \"Hello\" and \"monde\" maps to \"world\". In the seq2seq model, the decoder may implicitly select the corresponding information from the state passed by the decoder. The attention mechanism, however, makes this selection explicit.\n",
    "\n",
    "Attention is a generalized pooling method with bias alignment over inputs. The core component in the attention mechanism is the attention layer, or called attention for simplicity. An input of the attention layer is called a query. For a query, the attention layer returns the output based on its memory, which is a set of key-value pairs. To be more specific, assume a query $\\mathbf{q}\\in\\mathbb R^{d_q}$, and the memory contains $n$ key-value pairs, $(\\mathbf{k}_1, \\mathbf{v}_1), \\ldots, (\\mathbf{k}_n, \\mathbf{v}_n)$, with $\\mathbf{k}_i\\in\\mathbb R^{d_k}$, $\\mathbf{v}_i\\in\\mathbb R^{d_v}$. The attention layer then returns an output $\\mathbf o\\in\\mathbb R^{d_v}$ with the same shape as a value.\n",
    "\n",
    "![The attention layer returns an output based on the input query and its memory.](../img/attention.svg)\n",
    "\n",
    "To compute the output, we first assume there is a score function $\\alpha$ which measures the similarity between the query and a key. Then we compute all $n$ scores $a_1, \\ldots, a_n$ by\n",
    "\n",
    "$$a_i = \\alpha(\\mathbf q, \\mathbf k_i).$$\n",
    "\n",
    "Next we use softmax to obtain the attention weights\n",
    "\n",
    "$$b_1, \\ldots, b_n = \\textrm{softmax}(a_1, \\ldots, a_n).$$\n",
    "\n",
    "The output is then a weighted sum of the values\n",
    "\n",
    "$$\\mathbf o = \\sum_{i=1}^n b_i \\mathbf v_i.$$\n",
    "\n",
    "Different choices of the score function lead to different attention layers. We will discuss two commonly used attention layers in the rest of this section. Before diving into the implementation, we first introduce a masked version of the softmax operator and explain a specialized dot operator `nd.batched_dot`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Masked Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The masked softmax enables enforcing causality when computing attention weights.\n",
    "It takes the attention scores and a mask as input and filters out masked scores when computing the attention weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T22:51:39.259995Z",
     "start_time": "2019-07-25T22:51:39.226880Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"highlight\"><pre><span></span><span class=\"k\">def</span> <span class=\"nf\">_masked_softmax</span><span class=\"p\">(</span><span class=\"n\">F</span><span class=\"p\">,</span> <span class=\"n\">att_score</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"p\">):</span>\n",
       "    <span class=\"sd\">&quot;&quot;&quot;Ignore the masked elements when calculating the softmax</span>\n",
       "\n",
       "<span class=\"sd\">    Parameters</span>\n",
       "<span class=\"sd\">    ----------</span>\n",
       "<span class=\"sd\">    F : symbol or ndarray</span>\n",
       "<span class=\"sd\">    att_score : Symborl or NDArray</span>\n",
       "<span class=\"sd\">        Shape (batch_size, query_length, memory_length)</span>\n",
       "<span class=\"sd\">    mask : Symbol or NDArray or None</span>\n",
       "<span class=\"sd\">        Shape (batch_size, query_length, memory_length)</span>\n",
       "<span class=\"sd\">    Returns</span>\n",
       "<span class=\"sd\">    -------</span>\n",
       "<span class=\"sd\">    att_weights : Symborl or NDArray</span>\n",
       "<span class=\"sd\">        Shape (batch_size, query_length, memory_length)</span>\n",
       "<span class=\"sd\">    &quot;&quot;&quot;</span>\n",
       "    <span class=\"k\">if</span> <span class=\"n\">mask</span> <span class=\"ow\">is</span> <span class=\"ow\">not</span> <span class=\"bp\">None</span><span class=\"p\">:</span>\n",
       "        <span class=\"c1\"># Fill in the masked scores with a very small value</span>\n",
       "        <span class=\"n\">neg</span> <span class=\"o\">=</span> <span class=\"o\">-</span><span class=\"mf\">1e18</span>\n",
       "        <span class=\"k\">if</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"p\">(</span><span class=\"n\">dtype</span><span class=\"p\">)</span> <span class=\"o\">==</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">float16</span><span class=\"p\">:</span>\n",
       "            <span class=\"n\">neg</span> <span class=\"o\">=</span> <span class=\"o\">-</span><span class=\"mf\">1e4</span>\n",
       "        <span class=\"k\">else</span><span class=\"p\">:</span>\n",
       "            <span class=\"k\">try</span><span class=\"p\">:</span>\n",
       "                <span class=\"c1\"># if AMP (automatic mixed precision) is enabled, -1e18 will cause NaN.</span>\n",
       "                <span class=\"kn\">from</span> <span class=\"nn\">mxnet.contrib</span> <span class=\"kn\">import</span> <span class=\"n\">amp</span>\n",
       "                <span class=\"k\">if</span> <span class=\"n\">amp</span><span class=\"o\">.</span><span class=\"n\">amp</span><span class=\"o\">.</span><span class=\"n\">_amp_initialized</span><span class=\"p\">:</span>\n",
       "                    <span class=\"n\">neg</span> <span class=\"o\">=</span> <span class=\"o\">-</span><span class=\"mf\">1e4</span>\n",
       "            <span class=\"k\">except</span> <span class=\"ne\">ImportError</span><span class=\"p\">:</span>\n",
       "                <span class=\"k\">pass</span>\n",
       "        <span class=\"n\">att_score</span> <span class=\"o\">=</span> <span class=\"n\">F</span><span class=\"o\">.</span><span class=\"n\">where</span><span class=\"p\">(</span><span class=\"n\">mask</span><span class=\"p\">,</span> <span class=\"n\">att_score</span><span class=\"p\">,</span> <span class=\"n\">neg</span> <span class=\"o\">*</span> <span class=\"n\">F</span><span class=\"o\">.</span><span class=\"n\">ones_like</span><span class=\"p\">(</span><span class=\"n\">att_score</span><span class=\"p\">))</span>\n",
       "        <span class=\"n\">att_weights</span> <span class=\"o\">=</span> <span class=\"n\">F</span><span class=\"o\">.</span><span class=\"n\">softmax</span><span class=\"p\">(</span><span class=\"n\">att_score</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=-</span><span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">mask</span>\n",
       "    <span class=\"k\">else</span><span class=\"p\">:</span>\n",
       "        <span class=\"n\">att_weights</span> <span class=\"o\">=</span> <span class=\"n\">F</span><span class=\"o\">.</span><span class=\"n\">softmax</span><span class=\"p\">(</span><span class=\"n\">att_score</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=-</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n",
       "    <span class=\"k\">return</span> <span class=\"n\">att_weights</span>\n",
       "</pre></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "source(nlp.model.attention_cell._masked_softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct two examples, where each example is a 2-by-4 matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T22:51:39.273702Z",
     "start_time": "2019-07-25T22:51:39.261498Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[[0.49104288 0.50895715 0.         0.        ]\n",
       "  [0.4118593  0.58814067 0.         0.        ]]\n",
       "\n",
       " [[0.29469743 0.42473418 0.28056836 0.        ]\n",
       "  [0.2910362  0.47771612 0.23124772 0.        ]]]\n",
       "<NDArray 2x2x4 @cpu(0)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = nd.random.uniform(shape=(2,2,4))\n",
    "mask = nd.ones(shape=(2,2,4))\n",
    "mask[0, :, 2:] = 0\n",
    "mask[1, :, 3:] = 0\n",
    "nlp.model.attention_cell._masked_softmax(nd, data, mask, data.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The operator `nd.batched_dot` takes two inputs $X$ and $Y$ with shapes $(b, n, m)$ and $(b, m, k)$, respectively. It computes $b$ dot products, with `Z[i,:,:]=dot(X[i,:,:], Y[i,:,:]` for $i=1,\\ldots,n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T22:51:39.279752Z",
     "start_time": "2019-07-25T22:51:39.275333Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "4"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[[3. 3.]]\n",
       "\n",
       " [[3. 3.]]]\n",
       "<NDArray 2x1x2 @cpu(0)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nd.batch_dot(nd.ones((2,1,3)), nd.ones((2,3,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Dot Product Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The dot product assumes the query has the same dimension as the keys, namely $\\mathbf q, \\mathbf k_i \\in\\mathbb R^d$ for all $i$. It computes the score by an inner product between the query and a key, often then divided by $\\sqrt{d}$ to make the scores less sensitive to the dimension $d$. In other words,\n",
    "\n",
    "$$\\alpha(\\mathbf q, \\mathbf k) = \\langle \\mathbf q, \\mathbf k \\rangle /\\sqrt{d}.$$\n",
    "\n",
    "Assume $\\mathbf Q\\in\\mathbb R^{m\\times d}$ contains $m$ queries and $\\mathbf K\\in\\mathbb R^{n\\times d}$ has all $n$ keys. We can compute all $mn$ scores by\n",
    "\n",
    "$$\\alpha(\\mathbf Q, \\mathbf K) = \\mathbf Q \\mathbf K^T /\\sqrt{d}.$$\n",
    "\n",
    "Now let's implement this layer that supports a batch of queries and key-value pairs. In addition, it supports randomly dropping some attention weights as a regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T22:51:39.295021Z",
     "start_time": "2019-07-25T22:51:39.281339Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    },
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class DotProductAttention(nn.Block): \n",
    "    def __init__(self, dropout, **kwargs):\n",
    "        super(DotProductAttention, self).__init__(**kwargs)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    # query: (batch_size, #queries, d)\n",
    "    # key: (batch_size, #kv_pairs, d)\n",
    "    # value: (batch_size, #kv_pairs, dim_v)\n",
    "    # mask: (batch_size, #queries, #kv_pairs)\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        d = query.shape[-1]\n",
    "        # set transpose_b=True to swap the last two dimensions of key\n",
    "        scores = nd.batch_dot(query, key, transpose_b=True) / math.sqrt(d)\n",
    "        attention_weights = nlp.model.attention_cell._masked_softmax(mx.nd, scores, mask, scores.dtype)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        return nd.batch_dot(attention_weights, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In gluonnlp available as `nlp.model.DotProductAttentionCell`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Now we create two batches, and each batch has one query and 10 key-value pairs. \n",
    "We specify through `mask` that for the first batch, we will only pay attention to the first key-value pair, while for the second batch, we will check the first 6 key-value pairs. Therefore, though both batches have the same query and key-value pairs, we obtain different outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T22:51:39.306884Z",
     "start_time": "2019-07-25T22:51:39.296608Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[[0.5       ]\n",
       "  [0.7310586 ]\n",
       "  [0.880797  ]\n",
       "  [0.95257413]\n",
       "  [0.98201376]]\n",
       "\n",
       " [[1.5       ]\n",
       "  [2.492653  ]\n",
       "  [2.8448246 ]\n",
       "  [2.9476287 ]\n",
       "  [2.9813433 ]]]\n",
       "<NDArray 2x5x1 @cpu(0)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atten = DotProductAttention(dropout=0.5)\n",
    "atten.initialize()\n",
    "X = nd.broadcast_axis(nd.arange(5).reshape((1,5,1)), axis=0, size=2)\n",
    "mask = nd.ones(shape=(2,5,5))\n",
    "mask[0, :, 2:] = 0\n",
    "mask[1, :, 4:] = 0\n",
    "atten(X, X, X, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![Multi-head attention](../img/multi-head-attention.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "A multi-head attention layer consists of $h$ parallel attention layers, each one is called a head. For each head, we use three dense layers with hidden sizes $p_q$, $p_k$ and $p_v$ to project the queries, keys and values, respectively, before feeding into the attention layer. The outputs of these $h$ heads are concatenated and then projected by another dense layer.\n",
    "\n",
    "To be more specific, assume we have the learnable parameters\n",
    "$\\mathbf W_q^{(i)}\\in\\mathbb R^{p_q\\times d_q}$,\n",
    "$\\mathbf W_k^{(i)}\\in\\mathbb R^{p_k\\times d_k}$,\n",
    "and $\\mathbf W_v^{(i)}\\in\\mathbb R^{p_v\\times d_v}$,\n",
    " for $i=1,\\ldots,h$, and $\\mathbf W_o\\in\\mathbb R^{d_o\\times h p_v}$. Then the output for each head can be obtained by\n",
    "\n",
    "$$\\mathbf o^{(i)} = \\textrm{attention}(\\mathbf W_q^{(i)}\\mathbf q, \\mathbf W_k^{(i)}\\mathbf k,\\mathbf W_v^{(i)}\\mathbf v),$$\n",
    "\n",
    "where $\\text{attention}$ can be any attention layer introduced before. Since we already have learnable parameters, the simple dot product attention is used.\n",
    "\n",
    "Then we concatenate all outputs and project them to obtain the multi-head attention output\n",
    "\n",
    "$$\\mathbf o = \\mathbf W_o \\begin{bmatrix}\\mathbf o^{(1)}\\\\\\vdots\\\\\\mathbf o^{(h)}\\end{bmatrix}.$$\n",
    "\n",
    "In practice, we often use $p_q=p_k=p_v=d_o/h$. The hyper-parameters for a multi-head attention, therefore, contain the number heads $h$, and output feature size $d_o$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T22:51:39.338153Z",
     "start_time": "2019-07-25T22:51:39.308522Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    },
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Block):\n",
    "    def __init__(self, units, num_heads, dropout, **kwargs):  # units = d_o\n",
    "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
    "        assert units % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = DotProductAttention(dropout)\n",
    "        self.W_q = nn.Dense(units, use_bias=False, flatten=False)\n",
    "        self.W_k = nn.Dense(units, use_bias=False, flatten=False)\n",
    "        self.W_v = nn.Dense(units, use_bias=False, flatten=False)\n",
    "\n",
    "    # query, key, and value shape: (batch_size, num_items, dim)\n",
    "    # mask shape is (batch_size, query_length, memory_length)\n",
    "    def forward(self, query, key, value, mask):\n",
    "        # Project and transpose from (batch_size, num_items, units) to\n",
    "        # (batch_size * num_heads, num_items, p), where units = p * num_heads.\n",
    "        query, key, value = [transpose_qkv(X, self.num_heads) for X in (\n",
    "            self.W_q(query), self.W_k(key), self.W_v(value))]\n",
    "        if mask is not None:\n",
    "            # Replicate mask for each of the num_heads heads\n",
    "            mask = nd.broadcast_axis(nd.expand_dims(mask, axis=1),\n",
    "                                    axis=1, size=self.num_heads)\\\n",
    "                    .reshape(shape=(-1, 0, 0), reverse=True)\n",
    "        output = self.attention(query, key, value, mask)\n",
    "        # Transpose from (batch_size * num_heads, num_items, p) back to\n",
    "        # (batch_size, num_items, units)\n",
    "        return transpose_output(output, self.num_heads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Here are the definitions of the transpose functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T22:51:39.350422Z",
     "start_time": "2019-07-25T22:51:39.339726Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    },
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def transpose_qkv(X, num_heads):\n",
    "    # Shape after reshape: (batch_size, num_items, num_heads, p)\n",
    "    # 0 means copying the shape element, -1 means inferring its value\n",
    "    X = X.reshape((0, 0, num_heads, -1))\n",
    "    # Swap the num_items and the num_heads dimensions\n",
    "    X = X.transpose((0, 2, 1, 3))\n",
    "    # Merge the first two dimensions. Use reverse=True to infer\n",
    "    # shape from right to left\n",
    "    return X.reshape((-1, 0, 0), reverse=True)\n",
    "\n",
    "def transpose_output(X, num_heads):\n",
    "    # A reversed version of transpose_qkv\n",
    "    X = X.reshape((-1, num_heads, 0, 0), reverse=True)\n",
    "    X = X.transpose((0, 2, 1, 3))\n",
    "    return X.reshape((0, 0, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Create a multi-head attention with the output size $d_o$ equals to 100, the output will share the same batch size and sequence length as the input, but the last dimension will be equal to $d_o$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T22:51:39.362504Z",
     "start_time": "2019-07-25T22:51:39.351991Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "4"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 5, 100)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell = MultiHeadAttention(units=100, num_heads=10, dropout=0.5)\n",
    "cell.initialize()\n",
    "cell(X, X, X, mask).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In gluonnlp available as `nlp.model.MultiHeadAttentionCell`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Position-wise Feed-Forward Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The position-wise feed-forward network accepts a 3-dim input with shape (batch size, sequence length, feature size). It consists of two dense layers that applies to the last dimension, which means the same dense layers are used for each position item in the sequence, so called position-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T22:51:39.371148Z",
     "start_time": "2019-07-25T22:51:39.364086Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    },
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class PositionWiseFFN(nn.Block):\n",
    "    def __init__(self, units, hidden_size, **kwargs):\n",
    "        super(PositionWiseFFN, self).__init__(**kwargs)\n",
    "        self.ffn_1 = nn.Dense(hidden_size, flatten=False, activation='relu')\n",
    "        self.ffn_2 = nn.Dense(units, flatten=False)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.ffn_2(self.ffn_1(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Similar to the multi-head attention, the position-wise feed-forward network will only change the last dimension size of the input. In addition, if two items in the input sequence are identical, the according outputs will be identical as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T22:51:39.381092Z",
     "start_time": "2019-07-25T22:51:39.372773Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 0.00481268 -0.00251495  0.00616072  0.00038246]\n",
       " [ 0.00481268 -0.00251495  0.00616072  0.00038246]\n",
       " [ 0.00481268 -0.00251495  0.00616072  0.00038246]]\n",
       "<NDArray 3x4 @cpu(0)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ffn = PositionWiseFFN(4, 8)\n",
    "ffn.initialize()\n",
    "ffn(nd.ones((2, 3, 4)))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Add and Norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The input and the output of a multi-head attention layer or a position-wise feed-forward network are combined by a block that contains a residual structure and a layer normalization layer.\n",
    "\n",
    "Layer normalization is similar batch normalization, but the mean and variances are calculated along the last dimension, e.g `X.mean(axis=-1)` instead of the first batch dimension, e.g. `X.mean(axis=0)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T22:51:39.394341Z",
     "start_time": "2019-07-25T22:51:39.382683Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "7"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer norm: \n",
      "[[-0.99998  0.99998]\n",
      " [-0.99998  0.99998]]\n",
      "<NDArray 2x2 @cpu(0)> \n",
      "batch norm: \n",
      "[[-0.99998 -0.99998]\n",
      " [ 0.99998  0.99998]]\n",
      "<NDArray 2x2 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "layer = nn.LayerNorm()\n",
    "layer.initialize()\n",
    "batch = nn.BatchNorm()\n",
    "batch.initialize()\n",
    "X = nd.array([[1,2],[2,3]])\n",
    "# compute mean and variance from X in the training mode.\n",
    "with mx.autograd.record():\n",
    "    print('layer norm:',layer(X), '\\nbatch norm:', batch(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The connection block accepts two inputs $X$ and $Y$, the input and output of an other block. Within this connection block, we apply dropout on $Y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T22:51:39.402434Z",
     "start_time": "2019-07-25T22:51:39.395987Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "8"
    },
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class AddNorm(nn.Block):\n",
    "    def __init__(self, dropout, **kwargs):\n",
    "        super(AddNorm, self).__init__(**kwargs)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = nn.LayerNorm()\n",
    "\n",
    "    def forward(self, X, Y):\n",
    "        return self.norm(self.dropout(Y) + X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Due to the residual connection, $X$ and $Y$ should have the same shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T22:51:39.410959Z",
     "start_time": "2019-07-25T22:51:39.404022Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "9"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 4)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_norm = AddNorm(0.5)\n",
    "add_norm.initialize()\n",
    "add_norm(nd.ones((2,3,4)), nd.ones((2,3,4))).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Unlike the recurrent layer, both the multi-head attention layer and the position-wise feed-forward network compute the output of each item in the sequence independently. This property allows us to parallel the computation but is inefficient to model the sequence information. The transformer model therefore adds positional information into the input sequence.\n",
    "\n",
    "Assume $X\\in\\mathbb R^{l\\times d}$ is the embedding of an example, where $l$ is the sequence length and $d$ is the embedding size. This layer will create a positional encoding $P\\in\\mathbb R^{l\\times d}$ and output $P+X$, with $P$ defined as following:\n",
    "\n",
    "$$P_{i,2j} = \\sin(i/10000^{2j/d}),\\quad P_{i,2j+1} = \\cos(i/10000^{2j/d}),$$\n",
    "\n",
    "for $i=0,\\ldots,l-1$ and $j=0,\\ldots,\\lfloor(d-1)/2\\rfloor$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T22:51:39.438545Z",
     "start_time": "2019-07-25T22:51:39.412584Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "10"
    },
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def position_encoding_init(max_length, dim):\n",
    "    X = nd.arange(0, max_length).reshape((-1,1)) / nd.power(\n",
    "            10000, nd.arange(0, dim, 2)/dim)\n",
    "    position_weight = nd.zeros((max_length, dim))\n",
    "\n",
    "    position_weight[:, 0::2] = nd.sin(X)\n",
    "    position_weight[:, 1::2] = nd.cos(X)\n",
    "    return position_weight\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Block):\n",
    "    def __init__(self, units, dropout=0, max_len=1000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self._max_len = max_len\n",
    "        self._units = units\n",
    "        self.position_weight = position_encoding_init(max_len, units)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, X):\n",
    "        pos_seq = mx.nd.arange(X.shape[1]).expand_dims(0)\n",
    "        emb = nd.Embedding(pos_seq, self.position_weight, self._max_len, self._units)\n",
    "        return self.dropout(X + emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we visualize the position values for 4 dimensions. As can be seen, the 4th dimension has the same frequency as the 5th but with different offset. The 5th and 6th dimension have a lower frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T22:51:39.607203Z",
     "start_time": "2019-07-25T22:51:39.440313Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "11"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "pe = PositionalEncoding(20)\n",
    "pe.initialize()\n",
    "X = nd.zeros((1, 100, 20))\n",
    "Y = pe(X)\n",
    "_ = plt.plot(np.arange(100), Y.asnumpy()[0, :,4:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Now we define the transformer block for the encoder, which contains a multi-head attention layer, a position-wise feed-forward network, and two connection blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T22:51:39.618935Z",
     "start_time": "2019-07-25T22:51:39.608696Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "12"
    },
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Block):\n",
    "    def __init__(self, units, hidden_size, num_heads, dropout, **kwargs):\n",
    "        super(EncoderBlock, self).__init__(**kwargs)\n",
    "        self.attention = MultiHeadAttention(units, num_heads, dropout)\n",
    "        self.add_1 = AddNorm(dropout)\n",
    "        self.ffn = PositionWiseFFN(units, hidden_size)\n",
    "        self.add_2 = AddNorm(dropout)\n",
    "\n",
    "    def forward(self, X, mask):\n",
    "        Y = self.add_1(X, self.attention(X, X, X, mask))\n",
    "        return self.add_2(Y, self.ffn(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Due to the residual connections, this block will not change the input shape. It means the `units` argument should be equal to the input's last dimension size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T22:51:39.661373Z",
     "start_time": "2019-07-25T22:51:39.620283Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "13"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 100, 24)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_blk = EncoderBlock(24, 48, 8, 0.5)\n",
    "encoder_blk.initialize()\n",
    "mask = nd.ones(shape=(2, 100, 100))\n",
    "mask[0, :, 2:] = 0\n",
    "mask[1, :, 3:] = 0\n",
    "encoder_blk(nd.ones((2, 100, 24)), mask).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The encoder stacks $n$ blocks. Due to the residual connection again, the embedding layer size $d$ is same as the transformer block output size. Also note that we multiple the embedding output by $\\sqrt{d}$ to avoid its values are too small compared to positional encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T22:51:39.678946Z",
     "start_time": "2019-07-25T22:51:39.662970Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "14"
    },
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Block):\n",
    "    def __init__(self, vocab_size, units, hidden_size,\n",
    "                 num_heads, num_layers, dropout, **kwargs):\n",
    "        super(TransformerEncoder, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.embed = nn.Embedding(vocab_size, units)\n",
    "        self.pos_encoding = PositionalEncoding(units, dropout)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.blks.add(\n",
    "                EncoderBlock(units, hidden_size, num_heads, dropout))\n",
    "\n",
    "    def forward(self, X, mask, *args):\n",
    "        X = self.pos_encoding(self.embed(X) * math.sqrt(self.units))\n",
    "        for blk in self.blks:\n",
    "            X = blk(X, mask)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Create an encoder with two transformer blocks, whose hyper-parameters are same as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T22:51:39.708783Z",
     "start_time": "2019-07-25T22:51:39.680389Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "15"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 100, 24)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = TransformerEncoder(200, 24, 48, 8, 2, 0.5)\n",
    "encoder.initialize()\n",
    "encoder(nd.ones((2, 100)), mask).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![Predict at time step $t$ for a self-attention layer.](../img/self-attention-predict.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Let first look at how a decoder behaviors during predicting. Similar to the seq2seq model, we call $T$ forwards to generate a $T$ length sequence. At time step $t$, assume $\\mathbf x_t$ is the current input, i.e. the query. Then keys and values of the self-attention layer consist of the current query with all past queries $\\mathbf x_1, \\ldots, \\mathbf x_{t-1}$.\n",
    "\n",
    "During training, because the output for the $t$-query could depend all $T$ key-value pairs, which results in an inconsistent behavior than prediction. We can eliminate it by specifying the valid length to be $t$ for the $t$-th query.\n",
    "\n",
    "Another difference compared to the encoder transformer block is that the decoder block has an additional multi-head attention layer that accepts the encoder outputs as keys and values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T22:51:39.758267Z",
     "start_time": "2019-07-25T22:51:39.710256Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "16"
    },
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Block):\n",
    "    # i means it's the i-th block in the decoder\n",
    "    def __init__(self, units, hidden_size, num_heads, dropout, i, **kwargs):\n",
    "        super(DecoderBlock, self).__init__(**kwargs)\n",
    "        self.i = i\n",
    "        self.attention_1 = MultiHeadAttention(units, num_heads, dropout)\n",
    "        self.add_1 = AddNorm(dropout)\n",
    "        self.attention_2 = MultiHeadAttention(units, num_heads, dropout)\n",
    "        self.add_2 = AddNorm(dropout)\n",
    "        self.ffn = PositionWiseFFN(units, hidden_size)\n",
    "        self.add_3 = AddNorm(dropout)\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        enc_outputs, enc_valid_lengh = state[0], state[1]\n",
    "        # state[2][i] contains the past queries for this block\n",
    "        if state[2][self.i] is None:\n",
    "            key_values = X\n",
    "        else:\n",
    "            key_values = nd.concat(state[2][self.i], X, dim=1)\n",
    "        state[2][self.i] = key_values\n",
    "        if mx.autograd.is_training():\n",
    "            batch_size, seq_len, _ = X.shape\n",
    "            # shape: (batch_size, seq_len), the values in the j-th column\n",
    "            # are j+1\n",
    "            valid_length = nd.arange(\n",
    "                1, seq_len+1, ctx=X.context).tile((batch_size, 1))\n",
    "        else:\n",
    "            valid_length = None\n",
    "\n",
    "        X2 = self.attention_1(X, key_values, key_values, valid_length)\n",
    "        Y = self.add_1(X, X2)\n",
    "        Y2 = self.attention_2(Y, enc_outputs, enc_outputs, enc_valid_lengh)\n",
    "        Z = self.add_2(Y, Y2)\n",
    "        return self.add_3(Z, self.ffn(Z)), state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Similar to the encoder block, `units` should be equal to the last dimension size of $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T22:51:39.782815Z",
     "start_time": "2019-07-25T22:51:39.759761Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "17"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 100, 24)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_blk = DecoderBlock(24, 48, 8, 0.5, 0)\n",
    "decoder_blk.initialize()\n",
    "X = nd.ones((2, 100, 24))\n",
    "state = [encoder_blk(X, mask), mask, [None]]\n",
    "decoder_blk(X, state)[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The construction of the decoder is identical to the encoder except for the additional last dense layer to obtain confident scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T22:51:39.808939Z",
     "start_time": "2019-07-25T22:51:39.784460Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "18"
    },
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Block):\n",
    "    def __init__(self, vocab_size, units, hidden_size,\n",
    "                 num_heads, num_layers, dropout, **kwargs):\n",
    "        super(TransformerDecoder, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.num_layers = num_layers\n",
    "        self.embed = nn.Embedding(vocab_size, units)\n",
    "        self.pos_encoding = PositionalEncoding(units, dropout)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.blks.add(\n",
    "                DecoderBlock(units, hidden_size, num_heads, dropout, i))\n",
    "        self.dense = nn.Dense(vocab_size, flatten=False)\n",
    "\n",
    "    def init_state(self, enc_outputs, env_valid_lengh, *args):\n",
    "        return [enc_outputs, env_valid_lengh, [None]*self.num_layers]\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        X = self.pos_encoding(self.embed(X) * math.sqrt(self.units))\n",
    "        for blk in self.blks:\n",
    "            X, state = blk(X, state)\n",
    "        return self.dense(X), state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Use the Pretrained Transformer model\n",
    "\n",
    "Next, we load the Transformer model in GluonNLP model zoo, which returns the model + the source and target vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T22:51:43.940179Z",
     "start_time": "2019-07-25T22:51:39.810505Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Source Vocab: 36794 , #Target Vocab: 36794\n"
     ]
    }
   ],
   "source": [
    "import nmt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "wmt_transformer_model, wmt_src_vocab, wmt_tgt_vocab = \\\n",
    "    nlp.model.get_model('transformer_en_de_512',\n",
    "                        dataset_name='WMT2014',\n",
    "                        pretrained=True,\n",
    "                        ctx=ctx)\n",
    "# we are using mixed vocab of EN-DE, so the source and target language vocab are the same\n",
    "print('#Source Vocab:', len(wmt_src_vocab), ', #Target Vocab:', len(wmt_tgt_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T22:51:43.946818Z",
     "start_time": "2019-07-25T22:51:43.941949Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMTModel(\n",
      "  (encoder): TransformerEncoder(\n",
      "    (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "    (layer_norm): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=512)\n",
      "    (transformer_cells): HybridSequential(\n",
      "      (0): TransformerEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(512 -> 512, linear)\n",
      "          (proj_key): Dense(512 -> 512, linear)\n",
      "          (proj_value): Dense(512 -> 512, linear)\n",
      "        )\n",
      "        (proj): Dense(512 -> 512, linear)\n",
      "        (ffn): PositionwiseFFN(\n",
      "          (ffn_1): Dense(512 -> 2048, linear)\n",
      "          (activation): Activation(relu)\n",
      "          (ffn_2): Dense(2048 -> 512, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=512)\n",
      "        )\n",
      "        (layer_norm): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=512)\n",
      "      )\n",
      "      (1): TransformerEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(512 -> 512, linear)\n",
      "          (proj_key): Dense(512 -> 512, linear)\n",
      "          (proj_value): Dense(512 -> 512, linear)\n",
      "        )\n",
      "        (proj): Dense(512 -> 512, linear)\n",
      "        (ffn): PositionwiseFFN(\n",
      "          (ffn_1): Dense(512 -> 2048, linear)\n",
      "          (activation): Activation(relu)\n",
      "          (ffn_2): Dense(2048 -> 512, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=512)\n",
      "        )\n",
      "        (layer_norm): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=512)\n",
      "      )\n",
      "      (2): TransformerEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(512 -> 512, linear)\n",
      "          (proj_key): Dense(512 -> 512, linear)\n",
      "          (proj_value): Dense(512 -> 512, linear)\n",
      "        )\n",
      "        (proj): Dense(512 -> 512, linear)\n",
      "        (ffn): PositionwiseFFN(\n",
      "          (ffn_1): Dense(512 -> 2048, linear)\n",
      "          (activation): Activation(relu)\n",
      "          (ffn_2): Dense(2048 -> 512, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=512)\n",
      "        )\n",
      "        (layer_norm): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=512)\n",
      "      )\n",
      "      (3): TransformerEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(512 -> 512, linear)\n",
      "          (proj_key): Dense(512 -> 512, linear)\n",
      "          (proj_value): Dense(512 -> 512, linear)\n",
      "        )\n",
      "        (proj): Dense(512 -> 512, linear)\n",
      "        (ffn): PositionwiseFFN(\n",
      "          (ffn_1): Dense(512 -> 2048, linear)\n",
      "          (activation): Activation(relu)\n",
      "          (ffn_2): Dense(2048 -> 512, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=512)\n",
      "        )\n",
      "        (layer_norm): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=512)\n",
      "      )\n",
      "      (4): TransformerEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(512 -> 512, linear)\n",
      "          (proj_key): Dense(512 -> 512, linear)\n",
      "          (proj_value): Dense(512 -> 512, linear)\n",
      "        )\n",
      "        (proj): Dense(512 -> 512, linear)\n",
      "        (ffn): PositionwiseFFN(\n",
      "          (ffn_1): Dense(512 -> 2048, linear)\n",
      "          (activation): Activation(relu)\n",
      "          (ffn_2): Dense(2048 -> 512, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=512)\n",
      "        )\n",
      "        (layer_norm): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=512)\n",
      "      )\n",
      "      (5): TransformerEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(512 -> 512, linear)\n",
      "          (proj_key): Dense(512 -> 512, linear)\n",
      "          (proj_value): Dense(512 -> 512, linear)\n",
      "        )\n",
      "        (proj): Dense(512 -> 512, linear)\n",
      "        (ffn): PositionwiseFFN(\n",
      "          (ffn_1): Dense(512 -> 2048, linear)\n",
      "          (activation): Activation(relu)\n",
      "          (ffn_2): Dense(2048 -> 512, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=512)\n",
      "        )\n",
      "        (layer_norm): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=512)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "    (layer_norm): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=512)\n",
      "    (transformer_cells): HybridSequential(\n",
      "      (0): TransformerDecoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell_in): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(512 -> 512, linear)\n",
      "          (proj_key): Dense(512 -> 512, linear)\n",
      "          (proj_value): Dense(512 -> 512, linear)\n",
      "        )\n",
      "        (attention_cell_inter): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(512 -> 512, linear)\n",
      "          (proj_key): Dense(512 -> 512, linear)\n",
      "          (proj_value): Dense(512 -> 512, linear)\n",
      "        )\n",
      "        (proj_in): Dense(512 -> 512, linear)\n",
      "        (proj_inter): Dense(512 -> 512, linear)\n",
      "        (ffn): PositionwiseFFN(\n",
      "          (ffn_1): Dense(512 -> 2048, linear)\n",
      "          (activation): Activation(relu)\n",
      "          (ffn_2): Dense(2048 -> 512, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=512)\n",
      "        )\n",
      "        (layer_norm_in): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=512)\n",
      "        (layer_norm_inter): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=512)\n",
      "      )\n",
      "      (1): TransformerDecoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell_in): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(512 -> 512, linear)\n",
      "          (proj_key): Dense(512 -> 512, linear)\n",
      "          (proj_value): Dense(512 -> 512, linear)\n",
      "        )\n",
      "        (attention_cell_inter): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(512 -> 512, linear)\n",
      "          (proj_key): Dense(512 -> 512, linear)\n",
      "          (proj_value): Dense(512 -> 512, linear)\n",
      "        )\n",
      "        (proj_in): Dense(512 -> 512, linear)\n",
      "        (proj_inter): Dense(512 -> 512, linear)\n",
      "        (ffn): PositionwiseFFN(\n",
      "          (ffn_1): Dense(512 -> 2048, linear)\n",
      "          (activation): Activation(relu)\n",
      "          (ffn_2): Dense(2048 -> 512, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=512)\n",
      "        )\n",
      "        (layer_norm_in): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=512)\n",
      "        (layer_norm_inter): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=512)\n",
      "      )\n",
      "      (2): TransformerDecoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell_in): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(512 -> 512, linear)\n",
      "          (proj_key): Dense(512 -> 512, linear)\n",
      "          (proj_value): Dense(512 -> 512, linear)\n",
      "        )\n",
      "        (attention_cell_inter): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(512 -> 512, linear)\n",
      "          (proj_key): Dense(512 -> 512, linear)\n",
      "          (proj_value): Dense(512 -> 512, linear)\n",
      "        )\n",
      "        (proj_in): Dense(512 -> 512, linear)\n",
      "        (proj_inter): Dense(512 -> 512, linear)\n",
      "        (ffn): PositionwiseFFN(\n",
      "          (ffn_1): Dense(512 -> 2048, linear)\n",
      "          (activation): Activation(relu)\n",
      "          (ffn_2): Dense(2048 -> 512, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=512)\n",
      "        )\n",
      "        (layer_norm_in): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=512)\n",
      "        (layer_norm_inter): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=512)\n",
      "      )\n",
      "      (3): TransformerDecoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell_in): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(512 -> 512, linear)\n",
      "          (proj_key): Dense(512 -> 512, linear)\n",
      "          (proj_value): Dense(512 -> 512, linear)\n",
      "        )\n",
      "        (attention_cell_inter): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(512 -> 512, linear)\n",
      "          (proj_key): Dense(512 -> 512, linear)\n",
      "          (proj_value): Dense(512 -> 512, linear)\n",
      "        )\n",
      "        (proj_in): Dense(512 -> 512, linear)\n",
      "        (proj_inter): Dense(512 -> 512, linear)\n",
      "        (ffn): PositionwiseFFN(\n",
      "          (ffn_1): Dense(512 -> 2048, linear)\n",
      "          (activation): Activation(relu)\n",
      "          (ffn_2): Dense(2048 -> 512, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=512)\n",
      "        )\n",
      "        (layer_norm_in): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=512)\n",
      "        (layer_norm_inter): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=512)\n",
      "      )\n",
      "      (4): TransformerDecoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell_in): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(512 -> 512, linear)\n",
      "          (proj_key): Dense(512 -> 512, linear)\n",
      "          (proj_value): Dense(512 -> 512, linear)\n",
      "        )\n",
      "        (attention_cell_inter): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(512 -> 512, linear)\n",
      "          (proj_key): Dense(512 -> 512, linear)\n",
      "          (proj_value): Dense(512 -> 512, linear)\n",
      "        )\n",
      "        (proj_in): Dense(512 -> 512, linear)\n",
      "        (proj_inter): Dense(512 -> 512, linear)\n",
      "        (ffn): PositionwiseFFN(\n",
      "          (ffn_1): Dense(512 -> 2048, linear)\n",
      "          (activation): Activation(relu)\n",
      "          (ffn_2): Dense(2048 -> 512, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=512)\n",
      "        )\n",
      "        (layer_norm_in): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=512)\n",
      "        (layer_norm_inter): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=512)\n",
      "      )\n",
      "      (5): TransformerDecoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell_in): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(512 -> 512, linear)\n",
      "          (proj_key): Dense(512 -> 512, linear)\n",
      "          (proj_value): Dense(512 -> 512, linear)\n",
      "        )\n",
      "        (attention_cell_inter): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(512 -> 512, linear)\n",
      "          (proj_key): Dense(512 -> 512, linear)\n",
      "          (proj_value): Dense(512 -> 512, linear)\n",
      "        )\n",
      "        (proj_in): Dense(512 -> 512, linear)\n",
      "        (proj_inter): Dense(512 -> 512, linear)\n",
      "        (ffn): PositionwiseFFN(\n",
      "          (ffn_1): Dense(512 -> 2048, linear)\n",
      "          (activation): Activation(relu)\n",
      "          (ffn_2): Dense(2048 -> 512, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=512)\n",
      "        )\n",
      "        (layer_norm_in): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=512)\n",
      "        (layer_norm_inter): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=512)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (src_embed): HybridSequential(\n",
      "    (0): Embedding(36794 -> 512, float32)\n",
      "    (1): Dropout(p = 0.0, axes=())\n",
      "  )\n",
      "  (tgt_embed): HybridSequential(\n",
      "    (0): Embedding(36794 -> 512, float32)\n",
      "    (1): Dropout(p = 0.0, axes=())\n",
      "  )\n",
      "  (tgt_proj): Dense(512 -> 36794, linear)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(wmt_transformer_model) # Print the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The Transformer model architecture is shown as below:\n",
    "\n",
    "<img src=\"transformer.png\" width=\"480\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Load and Preprocess WMT 2014 Dataset\n",
    "\n",
    "We then load the newstest2014 segment in WMT 2014 English-German test dataset for evaluation purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Firstly, look at the WMT 2014 corpus. `GluonNLP` provides [WMT2014BPE](../../api/modules/data.rst#gluonnlp.data.WMT2014BPE)\n",
    "and [WMT2014](../../api/modules/data.rst#gluonnlp.data.WMT2014) classes. The former contains a BPE-tokenized dataset, while the later contains the raw text. Here, we use the former for scoring, and the latter for\n",
    "demonstrating actual translation.\n",
    "\n",
    "For the BPE, it is one way to convert words to sub-words. E.g, the word **cheapest** will be converted to **cheap@@** and **est**, and **sunnyvale** will be converted to **sunny@@** and **vale**. The representational ability of the vocabulary is greatly improved by using sub-words. This is a common trick in NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T22:51:43.974852Z",
     "start_time": "2019-07-25T22:51:43.948431Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source language en, Target language de\n",
      "Sample BPE tokens: \"('How the back of the plane is laid out - particularly whether seating is 9 or 10 ab@@ re@@ ast - is central to the economic performance claims being made for new \" mini-@@ j@@ umb@@ o \" jet designs .', 'Wie der hinter@@ e Teil des Flug@@ zeu@@ gs ausge@@ stal@@ tet ist - insbesondere ob es Reihen mit neun oder zehn Sit@@ zen gibt - , ist entscheidend für die angegebenen wirtschaftlichen Ergebnisse , die das neue \" Mini-@@ J@@ umb@@ o \" -@@ Design liefern soll .')\"\n",
      "Sample raw text: \"('How the back of the plane is laid out - particularly whether seating is 9 or 10 abreast - is central to the economic performance claims being made for new \"mini-jumbo\" jet designs.', 'Wie der hintere Teil des Flugzeugs ausgestaltet ist – insbesondere ob es Reihen mit neun oder zehn Sitzen gibt –, ist entscheidend für die angegebenen wirtschaftlichen Ergebnisse, die das neue „Mini-Jumbo“-Design liefern soll.')\"\n"
     ]
    }
   ],
   "source": [
    "import hyperparameters as hparams\n",
    "\n",
    "wmt_data_test = nlp.data.WMT2014BPE('newstest2014', # BPE: cheapest --> cheap@@, est\n",
    "                                    src_lang=hparams.src_lang,\n",
    "                                    tgt_lang=hparams.tgt_lang)\n",
    "print('Source language %s, Target language %s' % (hparams.src_lang, hparams.tgt_lang))\n",
    "print('Sample BPE tokens: \"{}\"'.format(wmt_data_test[14]))\n",
    "\n",
    "wmt_test_text = nlp.data.WMT2014('newstest2014',\n",
    "                                 src_lang=hparams.src_lang,\n",
    "                                 tgt_lang=hparams.tgt_lang)\n",
    "# For demo process, will only evaluate the prediction of the first 50 sentences\n",
    "wmt_data_test, wmt_test_text = gluon.data.SimpleDataset([wmt_data_test[i] for i in range(16)]), gluon.data.SimpleDataset([wmt_test_text[i] for i in range(16)])\n",
    "\n",
    "print('Sample raw text: \"{}\"'.format(wmt_test_text[14]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T22:51:43.980265Z",
     "start_time": "2019-07-25T22:51:43.976570Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample target sentence: \"Wie der hintere Teil des Flugzeugs ausgestaltet ist – insbesondere ob es Reihen mit neun oder zehn Sitzen gibt –, ist entscheidend für die angegebenen wirtschaftlichen Ergebnisse, die das neue „Mini-Jumbo“-Design liefern soll.\"\n"
     ]
    }
   ],
   "source": [
    "# Slice the target part of the dataset using .transform\n",
    "wmt_test_tgt_sentences = wmt_test_text.transform(lambda src, tgt: tgt)\n",
    "print('Sample target sentence: \"{}\"'.format(wmt_test_tgt_sentences[14]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We further process the dataset using the `.transform()` API. The preprocessing have the following 4 steps:\n",
    "\n",
    "1) Clip the source and target sequences\n",
    "\n",
    "2) Split the string input to a list of tokens\n",
    "\n",
    "3) Map the string token into its index in the vocabulary\n",
    "\n",
    "4) Append EOS token to source sentence and add BOS and EOS tokens to target sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T22:51:43.998720Z",
     "start_time": "2019-07-25T22:51:43.982059Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7300 21964 23833  1935 24004 11836  6698 11839  5565 25464 27950 22544\n",
      " 16202 24272     3] \n",
      " [    2  7300 21964 23833  1935 24004 29615  6698 11839  5565 25464 22297\n",
      " 27121 23712 20558     3]\n"
     ]
    }
   ],
   "source": [
    "import dataprocessor\n",
    "\n",
    "# wmt_transform_fn includes the four preprocessing steps mentioned above.\n",
    "wmt_transform_fn = dataprocessor.TrainValDataTransform(wmt_src_vocab, wmt_tgt_vocab)\n",
    "wmt_dataset_processed = wmt_data_test.transform(wmt_transform_fn, lazy=False)\n",
    "\n",
    "def get_length_index_fn():\n",
    "    global idx\n",
    "    idx = 0\n",
    "    def transform(src, tgt):\n",
    "        global idx\n",
    "        result = (src, tgt, len(src), len(tgt), idx)\n",
    "        idx += 1\n",
    "        return result\n",
    "    return transform\n",
    "\n",
    "wmt_data_test_with_len = wmt_dataset_processed.transform(get_length_index_fn(), lazy=False)\n",
    "# Five elements: Source Token Ids, Target Token Ids, Source Seq Length, Target Seq length, Index\n",
    "print(wmt_data_test_with_len[0][0], '\\n', wmt_data_test_with_len[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Creating `Sampler` and `DataLoader` for the `WMT 2014` Dataset\n",
    "\n",
    "Now, we have obtained the transformed datasets. The next step is to construct sampler and DataLoader. First, we need to construct batchify function, which pads and stacks sequences to form mini-batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T22:51:44.004478Z",
     "start_time": "2019-07-25T22:51:44.000434Z"
    },
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "wmt_test_batchify_fn = nlp.data.batchify.Tuple(\n",
    "    nlp.data.batchify.Pad(),                   # Source Token IDs\n",
    "    nlp.data.batchify.Pad(),                   # Target Token IDs\n",
    "    nlp.data.batchify.Stack(dtype='float32'),  # Source Sequence Length\n",
    "    nlp.data.batchify.Stack(dtype='float32'),  # Target Sequence Length\n",
    "    nlp.data.batchify.Stack())                 # Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* [Tuple](https://gluon-nlp.mxnet.io/api/modules/data.batchify.html?highlight=batchify#gluonnlp.data.batchify.Tuple) is the GluonNLP way of applying different batchify functions to each element of a dataset item. In this case, we are applying `Pad` to `src` and `tgt`, `Stack` to `len(src)` and `len(tgt)` with conversion to float32, and simple `Stack` to `idx` without type conversion.\n",
    "* [Pad](https://gluon-nlp.mxnet.io/api/modules/data.batchify.html?highlight=batchify#gluonnlp.data.batchify.Pad) takes the elements from all dataset items in a batch, and pad them according to the item of maximum length to form a padded matrix/tensor.\n",
    "* [Stack](https://gluon-nlp.mxnet.io/api/modules/data.batchify.html?highlight=batchify#gluonnlp.data.batchify.Stack) simply stacks all elements in a batch, and requires all elements to be of the same length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can then construct bucketing samplers, which generate batches by grouping sequences with similar lengths. Here, we use [FixedBucketSampler](https://gluon-nlp.mxnet.io/api/modules/data.html?highlight=fixedbucketsampler#gluonnlp.data.FixedBucketSampler). `FixedBucketSampler` aims to assign each data sample to a bucket based on its length. The buckets are determined automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    " Please refer to [BucketSampler](https://gluon-nlp.mxnet.io/api/notes/data_api.html) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T22:51:44.012578Z",
     "start_time": "2019-07-25T22:51:44.006123Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FixedBucketSampler:\n",
      "  sample_num=16, batch_num=9\n",
      "  key=[(26, 32), (40, 49), (54, 66)]\n",
      "  cnt=[9, 2, 5]\n",
      "  batch_size=[2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "wmt_test_batch_sampler = nlp.data.FixedBucketSampler(\n",
    "    lengths=wmt_data_test_with_len.transform(lambda src, tgt, src_len, tgt_len, idx: (src_len, tgt_len)), #(src, tgt)\n",
    "    num_buckets=3,\n",
    "    batch_size=2)\n",
    "print(wmt_test_batch_sampler.stats())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Given the samplers, we can use [DataLoader](https://mxnet.apache.org/versions/master/api/python/gluon/data.html#mxnet.gluon.data.DataLoader) to sample the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T22:51:44.382609Z",
     "start_time": "2019-07-25T22:51:44.014212Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of testing batches: 9\n"
     ]
    }
   ],
   "source": [
    "wmt_test_data_loader = gluon.data.DataLoader(\n",
    "    wmt_data_test_with_len,\n",
    "    batch_sampler=wmt_test_batch_sampler,\n",
    "    batchify_fn=wmt_test_batchify_fn,\n",
    "    num_workers=8)  # Note that we can use multi-processing\n",
    "print('Number of testing batches:', len(wmt_test_data_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Evaluate Transformer\n",
    "\n",
    "Next, we evaluate the performance of the model on the `newstest2014` dataset. We first define the `BeamSearchTranslator` to generate the translations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T22:51:44.392338Z",
     "start_time": "2019-07-25T22:51:44.385209Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beam Size = 4 , Lengh penalty Alpha= 0.6 , Length penalty K= 5\n"
     ]
    }
   ],
   "source": [
    "print('Beam Size =', hparams.beam_size, ', Lengh penalty Alpha=', hparams.lp_alpha, ', Length penalty K=', hparams.lp_k)\n",
    "wmt_translator = nmt.translation.BeamSearchTranslator(\n",
    "    model=wmt_transformer_model,\n",
    "    beam_size=hparams.beam_size,\n",
    "    scorer=nlp.model.BeamSearchScorer(alpha=hparams.lp_alpha, K=hparams.lp_k),\n",
    "    max_length=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Then we caculate the `loss` as well as the `bleu` score on the newstest2014 WMT 2014 English-German test dataset. This may take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T22:51:54.195616Z",
     "start_time": "2019-07-25T22:51:44.394169Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WMT14 EN-DE SOTA model test loss: 1.20; test bleu score: 35.49; time cost 9.79s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import utils\n",
    "\n",
    "eval_start_time = time.time()\n",
    "wmt_test_loss_function = nlp.loss.MaskedSoftmaxCELoss()\n",
    "wmt_test_loss_function.hybridize()\n",
    "wmt_detokenizer = nlp.data.SacreMosesDetokenizer()\n",
    "wmt_test_loss, wmt_test_translation_out = utils.evaluate(wmt_transformer_model,\n",
    "                                                         wmt_test_data_loader,\n",
    "                                                         wmt_test_loss_function,\n",
    "                                                         wmt_translator,\n",
    "                                                         wmt_tgt_vocab,\n",
    "                                                         wmt_detokenizer,\n",
    "                                                         ctx)\n",
    "wmt_test_bleu_score, _, _, _, _ = nmt.bleu.compute_bleu([wmt_test_tgt_sentences],\n",
    "                                                        wmt_test_translation_out,\n",
    "                                                        tokenized=False,\n",
    "                                                        tokenizer=hparams.bleu,\n",
    "                                                        split_compound_word=False,\n",
    "                                                        bpe=False)\n",
    "print('WMT14 EN-DE SOTA model test loss: %.2f; test bleu score: %.2f; time cost %.2fs' %(wmt_test_loss, wmt_test_bleu_score * 100, (time.time() - eval_start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T22:51:54.205905Z",
     "start_time": "2019-07-25T22:51:54.197461Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample translations:\n",
      "EN:\n",
      "Orlando Bloom and Miranda Kerr still love each other\n",
      "DE-Candidate:\n",
      "Orlando Bloom und Miranda Kerr lieben sich noch immer.\n",
      "DE-Reference:\n",
      "Orlando Bloom und Miranda Kerr lieben sich noch immer\n",
      "========\n"
     ]
    }
   ],
   "source": [
    "print('Sample translations:')\n",
    "num_pairs = 1\n",
    "\n",
    "for i in range(num_pairs):\n",
    "    print('EN:')\n",
    "    print(wmt_test_text[i][0])\n",
    "    print('DE-Candidate:')\n",
    "    print(wmt_test_translation_out[i])\n",
    "    print('DE-Reference:')\n",
    "    print(wmt_test_tgt_sentences[i])\n",
    "    print('========')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Translation Inference\n",
    "\n",
    "We herein show the actual translation example (EN-DE) when given a source language using the SOTA Transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T22:51:54.735927Z",
     "start_time": "2019-07-25T22:51:54.207525Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate the following English sentence into German:\n",
      "['We love language.']\n",
      "The German translation is:\n",
      "['Wir sind erfreut darüber, dass']\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "\n",
    "print('Translate the following English sentence into German:')\n",
    "\n",
    "sample_src_seq = 'We love language.'\n",
    "print('[\\'' + sample_src_seq + '\\']')\n",
    "sample_tgt_seq = utils.translate(wmt_translator, sample_src_seq, wmt_src_vocab, wmt_tgt_vocab, wmt_detokenizer,\n",
    "                                 ctx)\n",
    "print('The German translation is:')\n",
    "print(sample_tgt_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If you'd like to train your own transformer models, you may find the training scripts in our\n",
    "[scripts](https://github.com/dmlc/gluon-nlp/tree/master/scripts/machine_translation).\n",
    "\n",
    "## References\n",
    "\n",
    "[1] Vaswani, Ashish, et al. \"Attention is all you need.\" Advances in Neural Information Processing Systems. 2017."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
