{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Transformer Architecture and Machine Translation with the Transformer\n",
    "\n",
    "In this notebook, you will understand the Transformer architecture introduced in [Vaswani et al., 2017]. You will further learn how to load a pretrained Transformer model and translate a few sentences youself with the `BeamSearchTranslator`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Preparation\n",
    "\n",
    "We start with some usual preparation such as importing libraries and setting the environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T01:18:58.390004Z",
     "start_time": "2019-08-06T01:18:57.117533Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, nd\n",
    "from mxnet.gluon import nn\n",
    "import gluonnlp as nlp\n",
    "import d2l\n",
    "\n",
    "np.random.seed(100)\n",
    "random.seed(100)\n",
    "mx.random.seed(10000)\n",
    "ctx = d2l.try_gpu(0)\n",
    "\n",
    "mx.set_np_shape(True)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Attention Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "\n",
    "In :numref:`chapter_seq2seq`, we encode the source sequence input information in the recurrent unit state and then pass it to the decoder to generate the target sequence. A token in the target sequence may closely relate to some tokens in the source sequence instead of the whole source sequence. For example, when translating \"Hello world.\" to \"Bonjour le monde.\", \"Bonjour\" maps to \"Hello\" and \"monde\" maps to \"world\". In the seq2seq model, the decoder may implicitly select the corresponding information from the state passed by the decoder. The attention mechanism, however, makes this selection explicit.\n",
    "\n",
    "Attention is a generalized pooling method with bias alignment over inputs. The core component in the attention mechanism is the attention layer, or called attention for simplicity. An input of the attention layer is called a query. For a query, the attention layer returns the output based on its memory, which is a set of key-value pairs. To be more specific, assume a query $\\mathbf{q}\\in\\mathbb R^{d_q}$, and the memory contains $n$ key-value pairs, $(\\mathbf{k}_1, \\mathbf{v}_1), \\ldots, (\\mathbf{k}_n, \\mathbf{v}_n)$, with $\\mathbf{k}_i\\in\\mathbb R^{d_k}$, $\\mathbf{v}_i\\in\\mathbb R^{d_v}$. The attention layer then returns an output $\\mathbf o\\in\\mathbb R^{d_v}$ with the same shape as a value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"../img/attention.svg\" width=\"33%\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "\n",
    "\n",
    "To compute the output, we first assume there is a score function $\\alpha$ which measures the similarity between the query and a key. Then we compute all $n$ scores $a_1, \\ldots, a_n$ by\n",
    "\n",
    "$$a_i = \\alpha(\\mathbf q, \\mathbf k_i).$$\n",
    "\n",
    "Next we use softmax to obtain the attention weights\n",
    "\n",
    "$$b_1, \\ldots, b_n = \\textrm{softmax}(a_1, \\ldots, a_n).$$\n",
    "\n",
    "The output is then a weighted sum of the values\n",
    "\n",
    "$$\\mathbf o = \\sum_{i=1}^n b_i \\mathbf v_i.$$\n",
    "\n",
    "Different choices of the score function lead to different attention layers. We will discuss two commonly used attention layers in the rest of this section. Before diving into the implementation, we first introduce a masked version of the softmax operator and explain a specialized dot operator `nd.batched_dot`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Enforcing Causality: Masked Softmax\n",
    "\n",
    "<center>\n",
    "<img src=\"../img/causal-attention.png\" width=\"40%\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The masked softmax enables enforcing causality when computing attention weights.\n",
    "It takes the attention scores and a mask as input and filters out masked scores when computing the attention weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T01:19:07.112462Z",
     "start_time": "2019-08-06T01:19:07.072350Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "attention_scores = nd.random.uniform(shape=(2,2,4))\n",
    "mask = nd.ones(shape=(2,2,4))\n",
    "mask[0, :, 2:] = 0\n",
    "mask[1, :, 3:] = 0\n",
    "nlp.model.attention_cell._masked_softmax(nd, attention_scores, mask, attention_scores.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Now we create two batches, and each batch has one query and 10 key-value pairs. \n",
    "We specify through `mask` that for the first batch, we will only pay attention to the first key-value pair, while for the second batch, we will check the first 6 key-value pairs. Therefore, though both batches have the same query and key-value pairs, we obtain different outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Computing Attention Weights: Dot Product Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The dot product assumes the query has the same dimension as the keys, namely $\\mathbf q, \\mathbf k_i \\in\\mathbb R^d$ for all $i$. It computes the score by an inner product between the query and a key, often then divided by $\\sqrt{d}$ to make the scores less sensitive to the dimension $d$. In other words,\n",
    "\n",
    "$$\\alpha(\\mathbf q, \\mathbf k) = \\langle \\mathbf q, \\mathbf k \\rangle /\\sqrt{d}.$$\n",
    "\n",
    "Assume $\\mathbf Q\\in\\mathbb R^{m\\times d}$ contains $m$ queries and $\\mathbf K\\in\\mathbb R^{n\\times d}$ has all $n$ keys. We can compute all $mn$ scores by\n",
    "\n",
    "$$\\alpha(\\mathbf Q, \\mathbf K) = \\mathbf Q \\mathbf K^T /\\sqrt{d}.$$\n",
    "\n",
    "Now let's implement this layer that supports a batch of queries and key-value pairs. In addition, it supports randomly dropping some attention weights as a regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T01:19:13.111709Z",
     "start_time": "2019-08-06T01:19:13.106976Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class DotProductAttention(nn.Block): \n",
    "    def __init__(self, dropout, **kwargs):\n",
    "        super(DotProductAttention, self).__init__(**kwargs)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    # query: (batch_size, #queries, d)\n",
    "    # key: (batch_size, #kv_pairs, d)\n",
    "    # value: (batch_size, #kv_pairs, dim_v)\n",
    "    # mask: (batch_size, #queries, #kv_pairs)\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        d = query.shape[-1]\n",
    "        # set transpose_b=True to swap the last two dimensions of key\n",
    "        scores = nd.batch_dot(query, key, transpose_b=True) / math.sqrt(d)\n",
    "        attention_weights = nlp.model.attention_cell._masked_softmax(mx.nd, scores, mask, scores.dtype)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        return nd.batch_dot(attention_weights, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T01:19:15.485150Z",
     "start_time": "2019-08-06T01:19:15.477547Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "atten = DotProductAttention(dropout=0.5)\n",
    "atten.initialize()\n",
    "X = nd.broadcast_axis(nd.arange(5).reshape((1,5,1)), axis=0, size=2)\n",
    "mask = nd.ones(shape=(2,5,5))\n",
    "mask[0, :, 2:] = 0\n",
    "mask[1, :, 4:] = 0\n",
    "print(atten(X, X, X, mask))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "In gluonnlp available as `nlp.model.DotProductAttentionCell`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Multi-Head Attention\n",
    "\n",
    "<center>\n",
    "<img src=\"../img/multi-head-attention.svg\" width=\"50%\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "A multi-head attention layer consists of $h$ parallel attention layers, each one is called a head. For each head, we use three dense layers with hidden sizes $p_q$, $p_k$ and $p_v$ to project the queries, keys and values, respectively, before feeding into the attention layer. The outputs of these $h$ heads are concatenated and then projected by another dense layer.\n",
    "\n",
    "To be more specific, assume we have the learnable parameters\n",
    "$\\mathbf W_q^{(i)}\\in\\mathbb R^{p_q\\times d_q}$,\n",
    "$\\mathbf W_k^{(i)}\\in\\mathbb R^{p_k\\times d_k}$,\n",
    "and $\\mathbf W_v^{(i)}\\in\\mathbb R^{p_v\\times d_v}$,\n",
    " for $i=1,\\ldots,h$, and $\\mathbf W_o\\in\\mathbb R^{d_o\\times h p_v}$. Then the output for each head can be obtained by\n",
    "\n",
    "$$\\mathbf o^{(i)} = \\textrm{attention}(\\mathbf W_q^{(i)}\\mathbf q, \\mathbf W_k^{(i)}\\mathbf k,\\mathbf W_v^{(i)}\\mathbf v),$$\n",
    "\n",
    "where $\\text{attention}$ can be any attention layer introduced before. Since we already have learnable parameters, the simple dot product attention is used.\n",
    "\n",
    "Then we concatenate all outputs and project them to obtain the multi-head attention output\n",
    "\n",
    "$$\\mathbf o = \\mathbf W_o \\begin{bmatrix}\\mathbf o^{(1)}\\\\\\vdots\\\\\\mathbf o^{(h)}\\end{bmatrix}.$$\n",
    "\n",
    "In practice, we often use $p_q=p_k=p_v=d_o/h$. The hyper-parameters for a multi-head attention, therefore, contain the number heads $h$, and output feature size $d_o$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T01:19:20.552638Z",
     "start_time": "2019-08-06T01:19:20.538505Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Block):\n",
    "    def __init__(self, units, num_heads, dropout, **kwargs):  # units = d_o\n",
    "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
    "        assert units % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = DotProductAttention(dropout)\n",
    "        self.W_q = nn.Dense(units, use_bias=False, flatten=False)\n",
    "        self.W_k = nn.Dense(units, use_bias=False, flatten=False)\n",
    "        self.W_v = nn.Dense(units, use_bias=False, flatten=False)\n",
    "\n",
    "    # query, key, and value shape: (batch_size, num_items, dim)\n",
    "    # mask shape is (batch_size, query_length, memory_length)\n",
    "    def forward(self, query, key, value, mask):\n",
    "        # Project and transpose from (batch_size, num_items, units) to\n",
    "        # (batch_size * num_heads, num_items, p), where units = p * num_heads.\n",
    "        query, key, value = [transpose_qkv(X, self.num_heads) for X in (\n",
    "            self.W_q(query), self.W_k(key), self.W_v(value))]\n",
    "        if mask is not None:\n",
    "            # Replicate mask for each of the num_heads heads\n",
    "            mask = nd.broadcast_axis(nd.expand_dims(mask, axis=1),\n",
    "                                    axis=1, size=self.num_heads)\\\n",
    "                    .reshape(shape=(-1, 0, 0), reverse=True)\n",
    "        output = self.attention(query, key, value, mask)\n",
    "        # Transpose from (batch_size * num_heads, num_items, p) back to\n",
    "        # (batch_size, num_items, units)\n",
    "        return transpose_output(output, self.num_heads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Here are the definitions of the transpose functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T01:19:26.482976Z",
     "start_time": "2019-08-06T01:19:26.478468Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def transpose_qkv(X, num_heads):\n",
    "    # Shape after reshape: (batch_size, num_items, num_heads, p)\n",
    "    # 0 means copying the shape element, -1 means inferring its value\n",
    "    X = X.reshape((0, 0, num_heads, -1))\n",
    "    # Swap the num_items and the num_heads dimensions\n",
    "    X = X.transpose((0, 2, 1, 3))\n",
    "    # Merge the first two dimensions. Use reverse=True to infer\n",
    "    # shape from right to left\n",
    "    return X.reshape((-1, 0, 0), reverse=True)\n",
    "\n",
    "def transpose_output(X, num_heads):\n",
    "    # A reversed version of transpose_qkv\n",
    "    X = X.reshape((-1, num_heads, 0, 0), reverse=True)\n",
    "    X = X.transpose((0, 2, 1, 3))\n",
    "    return X.reshape((0, 0, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Create a multi-head attention with the output size $d_o$ equals to 100, the output will share the same batch size and sequence length as the input, but the last dimension will be equal to $d_o$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T01:19:28.016102Z",
     "start_time": "2019-08-06T01:19:28.006833Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "4"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "cell = MultiHeadAttention(units=100, num_heads=10, dropout=0.5)\n",
    "cell.initialize()\n",
    "cell(X, X, X, mask).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In gluonnlp available as `nlp.model.MultiHeadAttentionCell`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Transformer Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The Transformer model is also based on the encoder-decoder architecture. It,\n",
    "however, differs to the seq2seq model that the transformer replaces the\n",
    "recurrent layers in seq2seq with attention layers. To deal with sequential\n",
    "inputs, each item in the sequential is copied as the query, the key and the\n",
    "value as illustrated in :numref:`fig_self_attention`. It therefore outputs a same length\n",
    "sequential output. We call such an attention layer as a self-attention layer.\n",
    "\n",
    "\n",
    "<!-- Compared to a recurrent layer, output items of a self-attention layer can be computed in parallel and, therefore, it is easy to obtain a high-efficient implementation. -->\n",
    "\n",
    "The transformer architecture, with a comparison to the seq2seq model with\n",
    "attention, is shown in :numref:`fig_transformer`. These two models are similar to\n",
    "each other in overall: the source sequence embeddings are fed into $n$ repeated\n",
    "blocks. The outputs of the last block are then used as attention memory for the\n",
    "decoder.  The target sequence embeddings is similarly fed into $n$ repeated\n",
    "blocks in the decoder, and the final outputs are obtained by applying a dense\n",
    "layer with vocabulary size to the last block's outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"../img/transformer.png\" width=\"37%\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "It can also be seen that the transformer differs to the seq2seq with attention model in three major places:\n",
    "\n",
    "1. A recurrent layer in seq2seq is replaced with a transformer block. This block contains a self-attention layer (multi-head attention) and a network with two dense layers (position-wise FFN) for the encoder. For the decoder, another multi-head attention layer is used to take the encoder state.\n",
    "1. The encoder state is passed to every transformer block in the decoder, instead of using as an additional input of the first recurrent layer in seq2seq.\n",
    "1. Since the self-attention layer does not distinguish the item order in a sequence, a positional encoding layer is used to add sequential information into each sequence item.\n",
    "\n",
    "In the rest of this section, we will explain every new layer introduced by the transformer, and construct a model to train on the machine translation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Position-wise Feed-Forward Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The position-wise feed-forward network accepts a 3-dim input with shape (batch size, sequence length, feature size). It consists of two dense layers that applies to the last dimension, which means the same dense layers are used for each position item in the sequence, so called position-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T01:19:33.405267Z",
     "start_time": "2019-08-06T01:19:33.401301Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class PositionWiseFFN(nn.Block):\n",
    "    def __init__(self, units, hidden_size, **kwargs):\n",
    "        super(PositionWiseFFN, self).__init__(**kwargs)\n",
    "        # Dense layers is used for each position item in the sequence, so called position-wise. \n",
    "        self.ffn_1 = nn.Dense(hidden_size, flatten=False, activation='relu')\n",
    "        self.ffn_2 = nn.Dense(units, flatten=False)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # shape of X: (batch size, sequence length, feature size)\n",
    "        return self.ffn_2(self.ffn_1(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Similar to the multi-head attention, the position-wise feed-forward network will only change the last dimension size of the input. In addition, if two items in the input sequence are identical, the according outputs will be identical as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T01:19:34.733340Z",
     "start_time": "2019-08-06T01:19:34.725505Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ffn = PositionWiseFFN(4, 8)\n",
    "ffn.initialize()\n",
    "ffn(nd.ones((2, 3, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Add and Norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The input and the output of a multi-head attention layer or a position-wise feed-forward network are combined by a block that contains a residual structure and a layer normalization layer.\n",
    "\n",
    "Layer normalization is similar batch normalization, but the mean and variances are calculated along the last dimension, e.g `X.mean(axis=-1)` instead of the first batch dimension, e.g. `X.mean(axis=0)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T01:19:36.701220Z",
     "start_time": "2019-08-06T01:19:36.689630Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "7"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "layer = nn.LayerNorm()  # Normalize along channel-dimension\n",
    "layer.initialize()\n",
    "batch = nn.BatchNorm()  # Normalize along batch-dimension\n",
    "batch.initialize()\n",
    "X = nd.array([[1,2],[2,3]])\n",
    "# compute mean and variance from X in the training mode.\n",
    "with mx.autograd.record():\n",
    "    print('layer norm:',layer(X), '\\nbatch norm:', batch(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The connection block accepts two inputs $X$ and $Y$, the input and output of an other block. Within this connection block, we apply dropout on $Y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T01:19:37.869396Z",
     "start_time": "2019-08-06T01:19:37.865671Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "8"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class AddNorm(nn.Block):\n",
    "    def __init__(self, dropout, **kwargs):\n",
    "        super(AddNorm, self).__init__(**kwargs)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = nn.LayerNorm()\n",
    "\n",
    "    def forward(self, X, Y):\n",
    "        return self.norm(self.dropout(Y) + X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Due to the residual connection, $X$ and $Y$ should have the same shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T01:19:40.815469Z",
     "start_time": "2019-08-06T01:19:40.809125Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "9"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "add_norm = AddNorm(0.5)\n",
    "add_norm.initialize()\n",
    "add_norm(nd.ones((2,3,4)), nd.ones((2,3,4))).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Unlike the recurrent layer, both the multi-head attention layer and the position-wise feed-forward network compute the output of each item in the sequence independently. This property allows us to parallel the computation but is inefficient to model the sequence information. The transformer model therefore adds positional information into the input sequence.\n",
    "\n",
    "Assume $X\\in\\mathbb R^{l\\times d}$ is the embedding of an example, where $l$ is the sequence length and $d$ is the embedding size. This layer will create a positional encoding $P\\in\\mathbb R^{l\\times d}$ and output $P+X$, with $P$ defined as following:\n",
    "\n",
    "$$P_{i,2j} = \\sin(i/10000^{2j/d}),\\quad P_{i,2j+1} = \\cos(i/10000^{2j/d}),$$\n",
    "\n",
    "for $i=0,\\ldots,l-1$ and $j=0,\\ldots,\\lfloor(d-1)/2\\rfloor$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T01:19:44.447059Z",
     "start_time": "2019-08-06T01:19:44.440950Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "10"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def position_encoding_init(max_length, dim):\n",
    "    X = nd.arange(0, max_length).reshape((-1,1)) / nd.power(\n",
    "            10000, nd.arange(0, dim, 2)/dim)\n",
    "    position_weight = nd.zeros((max_length, dim))\n",
    "\n",
    "    position_weight[:, 0::2] = nd.sin(X)\n",
    "    position_weight[:, 1::2] = nd.cos(X)\n",
    "    return position_weight\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Block):\n",
    "    def __init__(self, units, dropout=0, max_len=1000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self._max_len = max_len\n",
    "        self._units = units\n",
    "        self.position_weight = position_encoding_init(max_len, units)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, X):\n",
    "        pos_seq = mx.nd.arange(X.shape[1]).expand_dims(0)\n",
    "        emb = nd.Embedding(pos_seq, self.position_weight, self._max_len, self._units)\n",
    "        return self.dropout(X + emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Now we visualize the position values for 4 dimensions. As can be seen, the 4th dimension has the same frequency as the 5th but with different offset. The 5th and 6th dimension have a lower frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T01:19:46.047460Z",
     "start_time": "2019-08-06T01:19:45.868882Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "11"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "pe = PositionalEncoding(20)\n",
    "pe.initialize()\n",
    "X = nd.zeros((1, 100, 20))\n",
    "Y = pe(X)\n",
    "_ = plt.plot(np.arange(100), Y.asnumpy()[0, :,4:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Transformer Encoder\n",
    "\n",
    "<img src=\"../img/self-attention.svg\" width=\"50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Now we define the transformer block for the encoder, which contains a multi-head attention layer, a position-wise feed-forward network, and two connection blocks.\n",
    "\n",
    "Due to the residual connections, this block will not change the input shape. It means the `units` argument should be equal to the input's last dimension size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T01:19:48.446913Z",
     "start_time": "2019-08-06T01:19:48.430390Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "12"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Block):\n",
    "    def __init__(self, units, hidden_size, num_heads, dropout, **kwargs):\n",
    "        super(EncoderBlock, self).__init__(**kwargs)\n",
    "        self.attention = MultiHeadAttention(units, num_heads, dropout)\n",
    "        self.add_1 = AddNorm(dropout)\n",
    "        self.ffn = PositionWiseFFN(units, hidden_size)\n",
    "        self.add_2 = AddNorm(dropout)\n",
    "\n",
    "    def forward(self, X, mask):\n",
    "        Y = self.add_1(X, self.attention(X, X, X, mask))\n",
    "        return self.add_2(Y, self.ffn(Y))\n",
    "    \n",
    "encoder_blk = EncoderBlock(24, 48, 8, 0.5)\n",
    "encoder_blk.initialize()\n",
    "mask = nd.ones(shape=(2, 100, 100))\n",
    "mask[0, :, 2:] = 0\n",
    "mask[1, :, 3:] = 0\n",
    "encoder_blk(nd.ones((2, 100, 24)), mask).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The encoder stacks $n$ blocks. Due to the residual connection again, the embedding layer size $d$ is same as the transformer block output size. Also note that we multiple the embedding output by $\\sqrt{d}$ to avoid its values are too small compared to positional encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T01:19:51.588232Z",
     "start_time": "2019-08-06T01:19:51.561649Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "14"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Block):\n",
    "    def __init__(self, vocab_size, units, hidden_size,\n",
    "                 num_heads, num_layers, dropout, **kwargs):\n",
    "        super(TransformerEncoder, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.embed = nn.Embedding(vocab_size, units)\n",
    "        self.pos_encoding = PositionalEncoding(units, dropout)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.blks.add(\n",
    "                EncoderBlock(units, hidden_size, num_heads, dropout))\n",
    "\n",
    "    def forward(self, X, mask, *args):\n",
    "        X = self.pos_encoding(self.embed(X) * math.sqrt(self.units))\n",
    "        for blk in self.blks:\n",
    "            X = blk(X, mask)\n",
    "        return X\n",
    "    \n",
    "encoder = TransformerEncoder(200, 24, 48, 8, 2, 0.5)\n",
    "encoder.initialize()\n",
    "encoder(nd.ones((2, 100)), mask).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Transformer-Decoder\n",
    "\n",
    "<center>\n",
    "<img src=\"../img/self-attention-predict.svg\" width=\"50%\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Let first look at how a decoder behaviors during predicting. Similar to the seq2seq model, we call $T$ forwards to generate a $T$ length sequence. At time step $t$, assume $\\mathbf x_t$ is the current input, i.e. the query. Then keys and values of the self-attention layer consist of the current query with all past queries $\\mathbf x_1, \\ldots, \\mathbf x_{t-1}$.\n",
    "\n",
    "During training, because the output for the $t$-query could depend all $T$ key-value pairs, which results in an inconsistent behavior than prediction. We can eliminate it by specifying the valid length to be $t$ for the $t$-th query.\n",
    "\n",
    "Another difference compared to the encoder transformer block is that the decoder block has an additional multi-head attention layer that accepts the encoder outputs as keys and values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T01:19:53.525380Z",
     "start_time": "2019-08-06T01:19:53.500161Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "16"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Block):\n",
    "    # i means it's the i-th block in the decoder\n",
    "    def __init__(self, units, hidden_size, num_heads, dropout, i, **kwargs):\n",
    "        super(DecoderBlock, self).__init__(**kwargs)\n",
    "        self.i = i\n",
    "        self.attention_1 = MultiHeadAttention(units, num_heads, dropout)\n",
    "        self.add_1 = AddNorm(dropout)\n",
    "        self.attention_2 = MultiHeadAttention(units, num_heads, dropout)\n",
    "        self.add_2 = AddNorm(dropout)\n",
    "        self.ffn = PositionWiseFFN(units, hidden_size)\n",
    "        self.add_3 = AddNorm(dropout)\n",
    "\n",
    "    def forward(self, X, state, dec_mask=None):\n",
    "        # key_values contains the past queries for this block\n",
    "        enc_outputs, enc_mask, key_values = state\n",
    "        key_values[self.i] = nd.concat(key_values[self.i], X, dim=1)\n",
    "        X2 = self.attention_1(X, key_values[self.i], key_values[self.i], dec_mask)\n",
    "        Y = self.add_1(X, X2)\n",
    "        Y2 = self.attention_2(Y, enc_outputs, enc_outputs, enc_mask)\n",
    "        Z = self.add_2(Y, Y2)\n",
    "        return self.add_3(Z, self.ffn(Z)), [enc_outputs, enc_mask, key_values]\n",
    "\n",
    "decoder_blk = DecoderBlock(24, 48, 8, 0.5, i=0)\n",
    "decoder_blk.initialize()\n",
    "\n",
    "X = nd.ones((2, 100, 24))\n",
    "enc_outputs = encoder_blk(X, mask)\n",
    "key_values = [nd.zeros(shape=(2,0,24))]\n",
    "state = [enc_outputs, mask, key_values]\n",
    "\n",
    "# Similar to the encoder block, `units` should be equal to the last dimension size of $X$.\n",
    "decoder_blk(X, state)[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The construction of the decoder is identical to the encoder except for the additional last dense layer to obtain confident scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T01:19:54.849191Z",
     "start_time": "2019-08-06T01:19:54.842781Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "18"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Block):\n",
    "    def __init__(self, vocab_size, units, hidden_size,\n",
    "                 num_heads, num_layers, dropout, **kwargs):\n",
    "        super(TransformerDecoder, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.num_layers = num_layers\n",
    "        self.embed = nn.Embedding(vocab_size, units)\n",
    "        self.pos_encoding = PositionalEncoding(units, dropout)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.blks.add(\n",
    "                DecoderBlock(units, hidden_size, num_heads, dropout, i))\n",
    "        self.dense = nn.Dense(vocab_size, flatten=False)\n",
    "\n",
    "    def init_state(self, enc_outputs, enc_valid_lengh, *args):\n",
    "        batch_size = enc_valid_length.shape[0]\n",
    "        empty_key_values = nd.zeros(shape=(batch_size, 0, self.units))\n",
    "        return [enc_outputs, enc_valid_lengh, [empty_key_values]*self.num_layers]\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        X = self.pos_encoding(self.embed(X) * math.sqrt(self.units))\n",
    "        for blk in self.blks:\n",
    "            X, state = blk(X, state)\n",
    "        return self.dense(X), state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Use the Pretrained Transformer model\n",
    "\n",
    "Next, we load the Transformer model in GluonNLP model zoo, which returns the model + the source and target vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T01:19:58.598851Z",
     "start_time": "2019-08-06T01:19:56.864366Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import nmt\n",
    "\n",
    "with mx.np_shape(False):\n",
    "    wmt_transformer_model, wmt_src_vocab, wmt_tgt_vocab = \\\n",
    "        nlp.model.get_model('transformer_en_de_512',\n",
    "                            dataset_name='WMT2014',\n",
    "                            pretrained=True,\n",
    "                            ctx=ctx)\n",
    "\n",
    "# we are using mixed vocab of EN-DE, so the source and target language vocab are the same\n",
    "print('#Source Vocab:', len(wmt_src_vocab), ', #Target Vocab:', len(wmt_tgt_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T01:20:00.162682Z",
     "start_time": "2019-08-06T01:20:00.156501Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "print(wmt_transformer_model) # Print the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Translation Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T01:20:40.826397Z",
     "start_time": "2019-08-06T01:20:40.429438Z"
    }
   },
   "outputs": [],
   "source": [
    "from gluonnlp.model import BeamSearchScorer, BeamSearchSampler\n",
    "import hyperparameters as hparams\n",
    "print('Beam Size =', hparams.beam_size,\n",
    "      ', Lengh penalty Alpha=', hparams.lp_alpha,\n",
    "      ', Length penalty K=', hparams.lp_k)\n",
    "\n",
    "def decode_logprob(step_input, states):\n",
    "    out, states, _ = wmt_transformer_model.decode_step(step_input, states)\n",
    "    return mx.nd.log_softmax(out), states\n",
    "\n",
    "\n",
    "sampler = BeamSearchSampler(\n",
    "    decoder=decode_logprob,\n",
    "    beam_size=hparams.beam_size,\n",
    "    eos_id=wmt_tgt_vocab.token_to_idx[wmt_tgt_vocab.eos_token],\n",
    "    scorer=nlp.model.BeamSearchScorer(alpha=hparams.lp_alpha, K=hparams.lp_k),\n",
    "    max_length=200)\n",
    "\n",
    "\n",
    "def translate(src_sentence, src_vocab, tgt_vocab):\n",
    "    src_sentence.append(src_vocab[src_vocab.eos_token])\n",
    "    src_nd = mx.nd.array([src_sentence], dtype=np.int32, ctx=ctx)\n",
    "    src_valid_length = mx.nd.array([src_nd.shape[1]], ctx=ctx)\n",
    "    encoder_outputs, _ = wmt_transformer_model.encode(src_nd, valid_length=src_valid_length)\n",
    "    decoder_states = wmt_transformer_model.decoder.init_state_from_encoder(encoder_outputs,\n",
    "                                                                 src_valid_length)\n",
    "    \n",
    "    bos_idx = tgt_vocab.token_to_idx[tgt_vocab.bos_token]\n",
    "    inputs = mx.nd.full(shape=(1,), ctx=ctx, dtype=np.float32, val=bos_idx)\n",
    "\n",
    "    samples, scores, sample_valid_length = sampler(inputs, decoder_states)\n",
    "\n",
    "    max_score_sample = samples[:, 0, :].asnumpy()\n",
    "    \n",
    "    sample_valid_length = sample_valid_length[:, 0].asnumpy()\n",
    "    translation_out = [\n",
    "        wmt_tgt_vocab.idx_to_token[ele] for ele in\n",
    "         max_score_sample[0][1:(sample_valid_length[0] - 1)]]\n",
    "    return nmt.bleu._bpe_to_words(translation_out)\n",
    "    \n",
    "print('Translating English to German:')\n",
    "\n",
    "sample_sentence = 'We love natural language processing .'.split()\n",
    "print(sample_sentence)\n",
    "sample_src_seq = wmt_src_vocab[sample_sentence]\n",
    "sample_tgt_seq = translate(sample_src_seq, wmt_src_vocab, wmt_tgt_vocab)\n",
    "print('The German translation is:')\n",
    "print(sample_tgt_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If you'd like to train your own transformer models, you may find the training scripts in our\n",
    "[scripts](https://github.com/dmlc/gluon-nlp/tree/master/scripts/machine_translation).\n",
    "\n",
    "## References\n",
    "\n",
    "[1] Vaswani, Ashish, et al. \"Attention is all you need.\" Advances in Neural Information Processing Systems. 2017."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
