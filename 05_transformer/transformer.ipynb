{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Transformer Architecture and Machine Translation with the Transformer\n",
    "\n",
    "In this notebook, you will understand the Transformer architecture introduced in [Vaswani et al., 2017]. You will further learn how to load a pretrained Transformer model and evaluate it on `newstest2014`. In addition, you are able to translate a few sentences youself with the `BeamSearchTranslator`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Preparation\n",
    "\n",
    "We start with some usual preparation such as importing libraries and setting the environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-31T21:19:32.163503Z",
     "start_time": "2019-07-31T21:19:31.885504Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, nd\n",
    "from mxnet.gluon import nn\n",
    "import gluonnlp as nlp\n",
    "\n",
    "np.random.seed(100)\n",
    "random.seed(100)\n",
    "mx.random.seed(10000)\n",
    "ctx = mx.gpu(0)\n",
    "\n",
    "from utils import setup_source, source\n",
    "setup_source()\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Attention Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "\n",
    "In :numref:`chapter_seq2seq`, we encode the source sequence input information in the recurrent unit state and then pass it to the decoder to generate the target sequence. A token in the target sequence may closely relate to some tokens in the source sequence instead of the whole source sequence. For example, when translating \"Hello world.\" to \"Bonjour le monde.\", \"Bonjour\" maps to \"Hello\" and \"monde\" maps to \"world\". In the seq2seq model, the decoder may implicitly select the corresponding information from the state passed by the decoder. The attention mechanism, however, makes this selection explicit.\n",
    "\n",
    "Attention is a generalized pooling method with bias alignment over inputs. The core component in the attention mechanism is the attention layer, or called attention for simplicity. An input of the attention layer is called a query. For a query, the attention layer returns the output based on its memory, which is a set of key-value pairs. To be more specific, assume a query $\\mathbf{q}\\in\\mathbb R^{d_q}$, and the memory contains $n$ key-value pairs, $(\\mathbf{k}_1, \\mathbf{v}_1), \\ldots, (\\mathbf{k}_n, \\mathbf{v}_n)$, with $\\mathbf{k}_i\\in\\mathbb R^{d_k}$, $\\mathbf{v}_i\\in\\mathbb R^{d_v}$. The attention layer then returns an output $\\mathbf o\\in\\mathbb R^{d_v}$ with the same shape as a value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![The attention layer returns an output based on the input query and its memory.](../img/attention.svg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "\n",
    "\n",
    "To compute the output, we first assume there is a score function $\\alpha$ which measures the similarity between the query and a key. Then we compute all $n$ scores $a_1, \\ldots, a_n$ by\n",
    "\n",
    "$$a_i = \\alpha(\\mathbf q, \\mathbf k_i).$$\n",
    "\n",
    "Next we use softmax to obtain the attention weights\n",
    "\n",
    "$$b_1, \\ldots, b_n = \\textrm{softmax}(a_1, \\ldots, a_n).$$\n",
    "\n",
    "The output is then a weighted sum of the values\n",
    "\n",
    "$$\\mathbf o = \\sum_{i=1}^n b_i \\mathbf v_i.$$\n",
    "\n",
    "Different choices of the score function lead to different attention layers. We will discuss two commonly used attention layers in the rest of this section. Before diving into the implementation, we first introduce a masked version of the softmax operator and explain a specialized dot operator `nd.batched_dot`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Dot Product Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The dot product assumes the query has the same dimension as the keys, namely $\\mathbf q, \\mathbf k_i \\in\\mathbb R^d$ for all $i$. It computes the score by an inner product between the query and a key, often then divided by $\\sqrt{d}$ to make the scores less sensitive to the dimension $d$. In other words,\n",
    "\n",
    "$$\\alpha(\\mathbf q, \\mathbf k) = \\langle \\mathbf q, \\mathbf k \\rangle /\\sqrt{d}.$$\n",
    "\n",
    "Assume $\\mathbf Q\\in\\mathbb R^{m\\times d}$ contains $m$ queries and $\\mathbf K\\in\\mathbb R^{n\\times d}$ has all $n$ keys. We can compute all $mn$ scores by\n",
    "\n",
    "$$\\alpha(\\mathbf Q, \\mathbf K) = \\mathbf Q \\mathbf K^T /\\sqrt{d}.$$\n",
    "\n",
    "Now let's implement this layer that supports a batch of queries and key-value pairs. In addition, it supports randomly dropping some attention weights as a regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T22:43:58.245264Z",
     "start_time": "2019-07-26T22:43:58.231491Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class DotProductAttention(nn.Block): \n",
    "    def __init__(self, dropout, **kwargs):\n",
    "        super(DotProductAttention, self).__init__(**kwargs)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    # query: (batch_size, #queries, d)\n",
    "    # key: (batch_size, #kv_pairs, d)\n",
    "    # value: (batch_size, #kv_pairs, dim_v)\n",
    "    # mask: (batch_size, #queries, #kv_pairs)\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        d = query.shape[-1]\n",
    "        # set transpose_b=True to swap the last two dimensions of key\n",
    "        scores = nd.batch_dot(query, key, transpose_b=True) / math.sqrt(d)\n",
    "        attention_weights = nlp.model.attention_cell._masked_softmax(mx.nd, scores, mask, scores.dtype)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        return nd.batch_dot(attention_weights, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In gluonnlp available as `nlp.model.DotProductAttentionCell`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Masked Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The masked softmax enables enforcing causality when computing attention weights.\n",
    "It takes the attention scores and a mask as input and filters out masked scores when computing the attention weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T22:43:58.266992Z",
     "start_time": "2019-07-26T22:43:58.246786Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "data = nd.random.uniform(shape=(2,2,4))\n",
    "mask = nd.ones(shape=(2,2,4))\n",
    "mask[0, :, 2:] = 0\n",
    "mask[1, :, 3:] = 0\n",
    "nlp.model.attention_cell._masked_softmax(nd, data, mask, data.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Now we create two batches, and each batch has one query and 10 key-value pairs. \n",
    "We specify through `mask` that for the first batch, we will only pay attention to the first key-value pair, while for the second batch, we will check the first 6 key-value pairs. Therefore, though both batches have the same query and key-value pairs, we obtain different outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Dot Product Attention Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T22:43:58.280370Z",
     "start_time": "2019-07-26T22:43:58.268506Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "atten = DotProductAttention(dropout=0.5)\n",
    "atten.initialize()\n",
    "X = nd.broadcast_axis(nd.arange(5).reshape((1,5,1)), axis=0, size=2)\n",
    "mask = nd.ones(shape=(2,5,5))\n",
    "mask[0, :, 2:] = 0\n",
    "mask[1, :, 4:] = 0\n",
    "atten(X, X, X, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![Multi-head attention](../img/multi-head-attention.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "A multi-head attention layer consists of $h$ parallel attention layers, each one is called a head. For each head, we use three dense layers with hidden sizes $p_q$, $p_k$ and $p_v$ to project the queries, keys and values, respectively, before feeding into the attention layer. The outputs of these $h$ heads are concatenated and then projected by another dense layer.\n",
    "\n",
    "To be more specific, assume we have the learnable parameters\n",
    "$\\mathbf W_q^{(i)}\\in\\mathbb R^{p_q\\times d_q}$,\n",
    "$\\mathbf W_k^{(i)}\\in\\mathbb R^{p_k\\times d_k}$,\n",
    "and $\\mathbf W_v^{(i)}\\in\\mathbb R^{p_v\\times d_v}$,\n",
    " for $i=1,\\ldots,h$, and $\\mathbf W_o\\in\\mathbb R^{d_o\\times h p_v}$. Then the output for each head can be obtained by\n",
    "\n",
    "$$\\mathbf o^{(i)} = \\textrm{attention}(\\mathbf W_q^{(i)}\\mathbf q, \\mathbf W_k^{(i)}\\mathbf k,\\mathbf W_v^{(i)}\\mathbf v),$$\n",
    "\n",
    "where $\\text{attention}$ can be any attention layer introduced before. Since we already have learnable parameters, the simple dot product attention is used.\n",
    "\n",
    "Then we concatenate all outputs and project them to obtain the multi-head attention output\n",
    "\n",
    "$$\\mathbf o = \\mathbf W_o \\begin{bmatrix}\\mathbf o^{(1)}\\\\\\vdots\\\\\\mathbf o^{(h)}\\end{bmatrix}.$$\n",
    "\n",
    "In practice, we often use $p_q=p_k=p_v=d_o/h$. The hyper-parameters for a multi-head attention, therefore, contain the number heads $h$, and output feature size $d_o$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T22:43:58.312238Z",
     "start_time": "2019-07-26T22:43:58.281852Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Block):\n",
    "    def __init__(self, units, num_heads, dropout, **kwargs):  # units = d_o\n",
    "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
    "        assert units % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = DotProductAttention(dropout)\n",
    "        self.W_q = nn.Dense(units, use_bias=False, flatten=False)\n",
    "        self.W_k = nn.Dense(units, use_bias=False, flatten=False)\n",
    "        self.W_v = nn.Dense(units, use_bias=False, flatten=False)\n",
    "\n",
    "    # query, key, and value shape: (batch_size, num_items, dim)\n",
    "    # mask shape is (batch_size, query_length, memory_length)\n",
    "    def forward(self, query, key, value, mask):\n",
    "        # Project and transpose from (batch_size, num_items, units) to\n",
    "        # (batch_size * num_heads, num_items, p), where units = p * num_heads.\n",
    "        query, key, value = [transpose_qkv(X, self.num_heads) for X in (\n",
    "            self.W_q(query), self.W_k(key), self.W_v(value))]\n",
    "        if mask is not None:\n",
    "            # Replicate mask for each of the num_heads heads\n",
    "            mask = nd.broadcast_axis(nd.expand_dims(mask, axis=1),\n",
    "                                    axis=1, size=self.num_heads)\\\n",
    "                    .reshape(shape=(-1, 0, 0), reverse=True)\n",
    "        output = self.attention(query, key, value, mask)\n",
    "        # Transpose from (batch_size * num_heads, num_items, p) back to\n",
    "        # (batch_size, num_items, units)\n",
    "        return transpose_output(output, self.num_heads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Here are the definitions of the transpose functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T22:43:58.324367Z",
     "start_time": "2019-07-26T22:43:58.313738Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def transpose_qkv(X, num_heads):\n",
    "    # Shape after reshape: (batch_size, num_items, num_heads, p)\n",
    "    # 0 means copying the shape element, -1 means inferring its value\n",
    "    X = X.reshape((0, 0, num_heads, -1))\n",
    "    # Swap the num_items and the num_heads dimensions\n",
    "    X = X.transpose((0, 2, 1, 3))\n",
    "    # Merge the first two dimensions. Use reverse=True to infer\n",
    "    # shape from right to left\n",
    "    return X.reshape((-1, 0, 0), reverse=True)\n",
    "\n",
    "def transpose_output(X, num_heads):\n",
    "    # A reversed version of transpose_qkv\n",
    "    X = X.reshape((-1, num_heads, 0, 0), reverse=True)\n",
    "    X = X.transpose((0, 2, 1, 3))\n",
    "    return X.reshape((0, 0, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Create a multi-head attention with the output size $d_o$ equals to 100, the output will share the same batch size and sequence length as the input, but the last dimension will be equal to $d_o$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T22:43:58.337501Z",
     "start_time": "2019-07-26T22:43:58.325903Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "4"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "cell = MultiHeadAttention(units=100, num_heads=10, dropout=0.5)\n",
    "cell.initialize()\n",
    "cell(X, X, X, mask).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In gluonnlp available as `nlp.model.MultiHeadAttentionCell`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Transformer Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The Transformer model is also based on the encoder-decoder architecture. It,\n",
    "however, differs to the seq2seq model that the transformer replaces the\n",
    "recurrent layers in seq2seq with attention layers. To deal with sequential\n",
    "inputs, each item in the sequential is copied as the query, the key and the\n",
    "value as illustrated in :numref:`fig_self_attention`. It therefore outputs a same length\n",
    "sequential output. We call such an attention layer as a self-attention layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![Self-attention architecture.](../img/self-attention.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "\n",
    "<!-- Compared to a recurrent layer, output items of a self-attention layer can be computed in parallel and, therefore, it is easy to obtain a high-efficient implementation. -->\n",
    "\n",
    "The transformer architecture, with a comparison to the seq2seq model with\n",
    "attention, is shown in :numref:`fig_transformer`. These two models are similar to\n",
    "each other in overall: the source sequence embeddings are fed into $n$ repeated\n",
    "blocks. The outputs of the last block are then used as attention memory for the\n",
    "decoder.  The target sequence embeddings is similarly fed into $n$ repeated\n",
    "blocks in the decoder, and the final outputs are obtained by applying a dense\n",
    "layer with vocabulary size to the last block's outputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![The transformer architecture.](../img/transformer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "It can also be seen that the transformer differs to the seq2seq with attention model in three major places:\n",
    "\n",
    "1. A recurrent layer in seq2seq is replaced with a transformer block. This block contains a self-attention layer (multi-head attention) and a network with two dense layers (position-wise FFN) for the encoder. For the decoder, another multi-head attention layer is used to take the encoder state.\n",
    "1. The encoder state is passed to every transformer block in the decoder, instead of using as an additional input of the first recurrent layer in seq2seq.\n",
    "1. Since the self-attention layer does not distinguish the item order in a sequence, a positional encoding layer is used to add sequential information into each sequence item.\n",
    "\n",
    "In the rest of this section, we will explain every new layer introduced by the transformer, and construct a model to train on the machine translation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Position-wise Feed-Forward Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The position-wise feed-forward network accepts a 3-dim input with shape (batch size, sequence length, feature size). It consists of two dense layers that applies to the last dimension, which means the same dense layers are used for each position item in the sequence, so called position-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T22:43:58.345986Z",
     "start_time": "2019-07-26T22:43:58.338968Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class PositionWiseFFN(nn.Block):\n",
    "    def __init__(self, units, hidden_size, **kwargs):\n",
    "        super(PositionWiseFFN, self).__init__(**kwargs)\n",
    "        self.ffn_1 = nn.Dense(hidden_size, flatten=False, activation='relu')\n",
    "        self.ffn_2 = nn.Dense(units, flatten=False)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.ffn_2(self.ffn_1(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Similar to the multi-head attention, the position-wise feed-forward network will only change the last dimension size of the input. In addition, if two items in the input sequence are identical, the according outputs will be identical as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T22:43:58.356818Z",
     "start_time": "2019-07-26T22:43:58.347449Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ffn = PositionWiseFFN(4, 8)\n",
    "ffn.initialize()\n",
    "ffn(nd.ones((2, 3, 4)))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Add and Norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The input and the output of a multi-head attention layer or a position-wise feed-forward network are combined by a block that contains a residual structure and a layer normalization layer.\n",
    "\n",
    "Layer normalization is similar batch normalization, but the mean and variances are calculated along the last dimension, e.g `X.mean(axis=-1)` instead of the first batch dimension, e.g. `X.mean(axis=0)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T22:43:58.371829Z",
     "start_time": "2019-07-26T22:43:58.358309Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "7"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "layer = nn.LayerNorm()\n",
    "layer.initialize()\n",
    "batch = nn.BatchNorm()\n",
    "batch.initialize()\n",
    "X = nd.array([[1,2],[2,3]])\n",
    "# compute mean and variance from X in the training mode.\n",
    "with mx.autograd.record():\n",
    "    print('layer norm:',layer(X), '\\nbatch norm:', batch(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The connection block accepts two inputs $X$ and $Y$, the input and output of an other block. Within this connection block, we apply dropout on $Y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T22:43:58.379590Z",
     "start_time": "2019-07-26T22:43:58.373286Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "8"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class AddNorm(nn.Block):\n",
    "    def __init__(self, dropout, **kwargs):\n",
    "        super(AddNorm, self).__init__(**kwargs)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = nn.LayerNorm()\n",
    "\n",
    "    def forward(self, X, Y):\n",
    "        return self.norm(self.dropout(Y) + X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Due to the residual connection, $X$ and $Y$ should have the same shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T22:43:58.388286Z",
     "start_time": "2019-07-26T22:43:58.381047Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "9"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "add_norm = AddNorm(0.5)\n",
    "add_norm.initialize()\n",
    "add_norm(nd.ones((2,3,4)), nd.ones((2,3,4))).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Unlike the recurrent layer, both the multi-head attention layer and the position-wise feed-forward network compute the output of each item in the sequence independently. This property allows us to parallel the computation but is inefficient to model the sequence information. The transformer model therefore adds positional information into the input sequence.\n",
    "\n",
    "Assume $X\\in\\mathbb R^{l\\times d}$ is the embedding of an example, where $l$ is the sequence length and $d$ is the embedding size. This layer will create a positional encoding $P\\in\\mathbb R^{l\\times d}$ and output $P+X$, with $P$ defined as following:\n",
    "\n",
    "$$P_{i,2j} = \\sin(i/10000^{2j/d}),\\quad P_{i,2j+1} = \\cos(i/10000^{2j/d}),$$\n",
    "\n",
    "for $i=0,\\ldots,l-1$ and $j=0,\\ldots,\\lfloor(d-1)/2\\rfloor$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T22:43:58.416262Z",
     "start_time": "2019-07-26T22:43:58.390242Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "10"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def position_encoding_init(max_length, dim):\n",
    "    X = nd.arange(0, max_length).reshape((-1,1)) / nd.power(\n",
    "            10000, nd.arange(0, dim, 2)/dim)\n",
    "    position_weight = nd.zeros((max_length, dim))\n",
    "\n",
    "    position_weight[:, 0::2] = nd.sin(X)\n",
    "    position_weight[:, 1::2] = nd.cos(X)\n",
    "    return position_weight\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Block):\n",
    "    def __init__(self, units, dropout=0, max_len=1000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self._max_len = max_len\n",
    "        self._units = units\n",
    "        self.position_weight = position_encoding_init(max_len, units)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, X):\n",
    "        pos_seq = mx.nd.arange(X.shape[1]).expand_dims(0)\n",
    "        emb = nd.Embedding(pos_seq, self.position_weight, self._max_len, self._units)\n",
    "        return self.dropout(X + emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we visualize the position values for 4 dimensions. As can be seen, the 4th dimension has the same frequency as the 5th but with different offset. The 5th and 6th dimension have a lower frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T22:43:58.599755Z",
     "start_time": "2019-07-26T22:43:58.417756Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "11"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "pe = PositionalEncoding(20)\n",
    "pe.initialize()\n",
    "X = nd.zeros((1, 100, 20))\n",
    "Y = pe(X)\n",
    "_ = plt.plot(np.arange(100), Y.asnumpy()[0, :,4:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Now we define the transformer block for the encoder, which contains a multi-head attention layer, a position-wise feed-forward network, and two connection blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T22:43:58.611408Z",
     "start_time": "2019-07-26T22:43:58.601231Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "12"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Block):\n",
    "    def __init__(self, units, hidden_size, num_heads, dropout, **kwargs):\n",
    "        super(EncoderBlock, self).__init__(**kwargs)\n",
    "        self.attention = MultiHeadAttention(units, num_heads, dropout)\n",
    "        self.add_1 = AddNorm(dropout)\n",
    "        self.ffn = PositionWiseFFN(units, hidden_size)\n",
    "        self.add_2 = AddNorm(dropout)\n",
    "\n",
    "    def forward(self, X, mask):\n",
    "        Y = self.add_1(X, self.attention(X, X, X, mask))\n",
    "        return self.add_2(Y, self.ffn(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Due to the residual connections, this block will not change the input shape. It means the `units` argument should be equal to the input's last dimension size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T22:43:58.658663Z",
     "start_time": "2019-07-26T22:43:58.612893Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "13"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "encoder_blk = EncoderBlock(24, 48, 8, 0.5)\n",
    "encoder_blk.initialize()\n",
    "mask = nd.ones(shape=(2, 100, 100))\n",
    "mask[0, :, 2:] = 0\n",
    "mask[1, :, 3:] = 0\n",
    "encoder_blk(nd.ones((2, 100, 24)), mask).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The encoder stacks $n$ blocks. Due to the residual connection again, the embedding layer size $d$ is same as the transformer block output size. Also note that we multiple the embedding output by $\\sqrt{d}$ to avoid its values are too small compared to positional encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T22:43:58.676019Z",
     "start_time": "2019-07-26T22:43:58.660123Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "14"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Block):\n",
    "    def __init__(self, vocab_size, units, hidden_size,\n",
    "                 num_heads, num_layers, dropout, **kwargs):\n",
    "        super(TransformerEncoder, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.embed = nn.Embedding(vocab_size, units)\n",
    "        self.pos_encoding = PositionalEncoding(units, dropout)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.blks.add(\n",
    "                EncoderBlock(units, hidden_size, num_heads, dropout))\n",
    "\n",
    "    def forward(self, X, mask, *args):\n",
    "        X = self.pos_encoding(self.embed(X) * math.sqrt(self.units))\n",
    "        for blk in self.blks:\n",
    "            X = blk(X, mask)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Create an encoder with two transformer blocks, whose hyper-parameters are same as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T22:43:58.714583Z",
     "start_time": "2019-07-26T22:43:58.677450Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "15"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "encoder = TransformerEncoder(200, 24, 48, 8, 2, 0.5)\n",
    "encoder.initialize()\n",
    "encoder(nd.ones((2, 100)), mask).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![Predict at time step $t$ for a self-attention layer.](../img/self-attention-predict.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Let first look at how a decoder behaviors during predicting. Similar to the seq2seq model, we call $T$ forwards to generate a $T$ length sequence. At time step $t$, assume $\\mathbf x_t$ is the current input, i.e. the query. Then keys and values of the self-attention layer consist of the current query with all past queries $\\mathbf x_1, \\ldots, \\mathbf x_{t-1}$.\n",
    "\n",
    "During training, because the output for the $t$-query could depend all $T$ key-value pairs, which results in an inconsistent behavior than prediction. We can eliminate it by specifying the valid length to be $t$ for the $t$-th query.\n",
    "\n",
    "Another difference compared to the encoder transformer block is that the decoder block has an additional multi-head attention layer that accepts the encoder outputs as keys and values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T22:43:58.764361Z",
     "start_time": "2019-07-26T22:43:58.716013Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "16"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Block):\n",
    "    # i means it's the i-th block in the decoder\n",
    "    def __init__(self, units, hidden_size, num_heads, dropout, i, **kwargs):\n",
    "        super(DecoderBlock, self).__init__(**kwargs)\n",
    "        self.i = i\n",
    "        self.attention_1 = MultiHeadAttention(units, num_heads, dropout)\n",
    "        self.add_1 = AddNorm(dropout)\n",
    "        self.attention_2 = MultiHeadAttention(units, num_heads, dropout)\n",
    "        self.add_2 = AddNorm(dropout)\n",
    "        self.ffn = PositionWiseFFN(units, hidden_size)\n",
    "        self.add_3 = AddNorm(dropout)\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        enc_outputs, enc_valid_lengh = state[0], state[1]\n",
    "        # state[2][i] contains the past queries for this block\n",
    "        if state[2][self.i] is None:\n",
    "            key_values = X\n",
    "        else:\n",
    "            key_values = nd.concat(state[2][self.i], X, dim=1)\n",
    "        state[2][self.i] = key_values\n",
    "        if mx.autograd.is_training():\n",
    "            batch_size, seq_len, _ = X.shape\n",
    "            # shape: (batch_size, seq_len), the values in the j-th column\n",
    "            # are j+1\n",
    "            valid_length = nd.arange(\n",
    "                1, seq_len+1, ctx=X.context).tile((batch_size, 1))\n",
    "        else:\n",
    "            valid_length = None\n",
    "\n",
    "        X2 = self.attention_1(X, key_values, key_values, valid_length)\n",
    "        Y = self.add_1(X, X2)\n",
    "        Y2 = self.attention_2(Y, enc_outputs, enc_outputs, enc_valid_lengh)\n",
    "        Z = self.add_2(Y, Y2)\n",
    "        return self.add_3(Z, self.ffn(Z)), state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Similar to the encoder block, `units` should be equal to the last dimension size of $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T22:43:58.795296Z",
     "start_time": "2019-07-26T22:43:58.765798Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "17"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "decoder_blk = DecoderBlock(24, 48, 8, 0.5, 0)\n",
    "decoder_blk.initialize()\n",
    "X = nd.ones((2, 100, 24))\n",
    "state = [encoder_blk(X, mask), mask, [None]]\n",
    "decoder_blk(X, state)[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The construction of the decoder is identical to the encoder except for the additional last dense layer to obtain confident scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T22:43:58.820800Z",
     "start_time": "2019-07-26T22:43:58.796720Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "18"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Block):\n",
    "    def __init__(self, vocab_size, units, hidden_size,\n",
    "                 num_heads, num_layers, dropout, **kwargs):\n",
    "        super(TransformerDecoder, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.num_layers = num_layers\n",
    "        self.embed = nn.Embedding(vocab_size, units)\n",
    "        self.pos_encoding = PositionalEncoding(units, dropout)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.blks.add(\n",
    "                DecoderBlock(units, hidden_size, num_heads, dropout, i))\n",
    "        self.dense = nn.Dense(vocab_size, flatten=False)\n",
    "\n",
    "    def init_state(self, enc_outputs, env_valid_lengh, *args):\n",
    "        return [enc_outputs, env_valid_lengh, [None]*self.num_layers]\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        X = self.pos_encoding(self.embed(X) * math.sqrt(self.units))\n",
    "        for blk in self.blks:\n",
    "            X, state = blk(X, state)\n",
    "        return self.dense(X), state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Use the Pretrained Transformer model\n",
    "\n",
    "Next, we load the Transformer model in GluonNLP model zoo, which returns the model + the source and target vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T22:44:03.758498Z",
     "start_time": "2019-07-26T22:43:58.822210Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import nmt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "wmt_transformer_model, wmt_src_vocab, wmt_tgt_vocab = \\\n",
    "    nlp.model.get_model('transformer_en_de_512',\n",
    "                        dataset_name='WMT2014',\n",
    "                        pretrained=True,\n",
    "                        ctx=ctx)\n",
    "# we are using mixed vocab of EN-DE, so the source and target language vocab are the same\n",
    "print('#Source Vocab:', len(wmt_src_vocab), ', #Target Vocab:', len(wmt_tgt_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T22:44:03.766228Z",
     "start_time": "2019-07-26T22:44:03.760043Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "print(wmt_transformer_model) # Print the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Load and Preprocess WMT 2014 Dataset\n",
    "\n",
    "We then load the newstest2014 segment in WMT 2014 English-German test dataset for evaluation purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Firstly, look at the WMT 2014 corpus. `GluonNLP` provides [WMT2014BPE](../../api/modules/data.rst#gluonnlp.data.WMT2014BPE)\n",
    "and [WMT2014](../../api/modules/data.rst#gluonnlp.data.WMT2014) classes. The former contains a BPE-tokenized dataset, while the later contains the raw text. Here, we use the former for scoring, and the latter for\n",
    "demonstrating actual translation.\n",
    "\n",
    "For the BPE, it is one way to convert words to sub-words. E.g, the word **cheapest** will be converted to **cheap@@** and **est**, and **sunnyvale** will be converted to **sunny@@** and **vale**. The representational ability of the vocabulary is greatly improved by using sub-words. This is a common trick in NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T22:44:03.796310Z",
     "start_time": "2019-07-26T22:44:03.767712Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import hyperparameters as hparams\n",
    "\n",
    "wmt_data_test = nlp.data.WMT2014BPE('newstest2014', # BPE: cheapest --> cheap@@, est\n",
    "                                    src_lang=hparams.src_lang,\n",
    "                                    tgt_lang=hparams.tgt_lang)\n",
    "print('Source language %s, Target language %s' % (hparams.src_lang, hparams.tgt_lang))\n",
    "print('Sample BPE tokens: \"{}\"'.format(wmt_data_test[14]))\n",
    "\n",
    "wmt_test_text = nlp.data.WMT2014('newstest2014',\n",
    "                                 src_lang=hparams.src_lang,\n",
    "                                 tgt_lang=hparams.tgt_lang)\n",
    "# For demo process, will only evaluate the prediction of the first 50 sentences\n",
    "wmt_data_test, wmt_test_text = gluon.data.SimpleDataset([wmt_data_test[i] for i in range(16)]), gluon.data.SimpleDataset([wmt_test_text[i] for i in range(16)])\n",
    "\n",
    "print('Sample raw text: \"{}\"'.format(wmt_test_text[14]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T22:44:03.801282Z",
     "start_time": "2019-07-26T22:44:03.797876Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Slice the target part of the dataset using .transform\n",
    "wmt_test_tgt_sentences = wmt_test_text.transform(lambda src, tgt: tgt)\n",
    "print('Sample target sentence: \"{}\"'.format(wmt_test_tgt_sentences[14]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We further process the dataset using the `.transform()` API. The preprocessing have the following 4 steps:\n",
    "\n",
    "1) Clip the source and target sequences\n",
    "\n",
    "2) Split the string input to a list of tokens\n",
    "\n",
    "3) Map the string token into its index in the vocabulary\n",
    "\n",
    "4) Append EOS token to source sentence and add BOS and EOS tokens to target sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T22:44:03.818884Z",
     "start_time": "2019-07-26T22:44:03.802809Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import dataprocessor\n",
    "\n",
    "# wmt_transform_fn includes the four preprocessing steps mentioned above.\n",
    "wmt_transform_fn = dataprocessor.TrainValDataTransform(wmt_src_vocab, wmt_tgt_vocab)\n",
    "wmt_dataset_processed = wmt_data_test.transform(wmt_transform_fn, lazy=False)\n",
    "\n",
    "def get_length_index_fn():\n",
    "    global idx\n",
    "    idx = 0\n",
    "    def transform(src, tgt):\n",
    "        global idx\n",
    "        result = (src, tgt, len(src), len(tgt), idx)\n",
    "        idx += 1\n",
    "        return result\n",
    "    return transform\n",
    "\n",
    "wmt_data_test_with_len = wmt_dataset_processed.transform(get_length_index_fn(), lazy=False)\n",
    "# Five elements: Source Token Ids, Target Token Ids, Source Seq Length, Target Seq length, Index\n",
    "print(wmt_data_test_with_len[0][0], '\\n', wmt_data_test_with_len[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Creating `Sampler` and `DataLoader` for the `WMT 2014` Dataset\n",
    "\n",
    "Now, we have obtained the transformed datasets. The next step is to construct sampler and DataLoader. First, we need to construct batchify function, which pads and stacks sequences to form mini-batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T22:44:03.824094Z",
     "start_time": "2019-07-26T22:44:03.820385Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "wmt_test_batchify_fn = nlp.data.batchify.Tuple(\n",
    "    nlp.data.batchify.Pad(),                   # Source Token IDs\n",
    "    nlp.data.batchify.Pad(),                   # Target Token IDs\n",
    "    nlp.data.batchify.Stack(dtype='float32'),  # Source Sequence Length\n",
    "    nlp.data.batchify.Stack(dtype='float32'),  # Target Sequence Length\n",
    "    nlp.data.batchify.Stack())                 # Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* [Tuple](https://gluon-nlp.mxnet.io/api/modules/data.batchify.html?highlight=batchify#gluonnlp.data.batchify.Tuple) is the GluonNLP way of applying different batchify functions to each element of a dataset item. In this case, we are applying `Pad` to `src` and `tgt`, `Stack` to `len(src)` and `len(tgt)` with conversion to float32, and simple `Stack` to `idx` without type conversion.\n",
    "* [Pad](https://gluon-nlp.mxnet.io/api/modules/data.batchify.html?highlight=batchify#gluonnlp.data.batchify.Pad) takes the elements from all dataset items in a batch, and pad them according to the item of maximum length to form a padded matrix/tensor.\n",
    "* [Stack](https://gluon-nlp.mxnet.io/api/modules/data.batchify.html?highlight=batchify#gluonnlp.data.batchify.Stack) simply stacks all elements in a batch, and requires all elements to be of the same length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can then construct bucketing samplers, which generate batches by grouping sequences with similar lengths. Here, we use [FixedBucketSampler](https://gluon-nlp.mxnet.io/api/modules/data.html?highlight=fixedbucketsampler#gluonnlp.data.FixedBucketSampler). `FixedBucketSampler` aims to assign each data sample to a bucket based on its length. The buckets are determined automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    " Please refer to [BucketSampler](https://gluon-nlp.mxnet.io/api/notes/data_api.html) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T22:44:03.831347Z",
     "start_time": "2019-07-26T22:44:03.826385Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "wmt_test_batch_sampler = nlp.data.FixedBucketSampler(\n",
    "    lengths=wmt_data_test_with_len.transform(lambda src, tgt, src_len, tgt_len, idx: (src_len, tgt_len)), #(src, tgt)\n",
    "    num_buckets=3,\n",
    "    batch_size=2)\n",
    "print(wmt_test_batch_sampler.stats())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Given the samplers, we can use [DataLoader](https://mxnet.apache.org/versions/master/api/python/gluon/data.html#mxnet.gluon.data.DataLoader) to sample the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T22:44:04.236760Z",
     "start_time": "2019-07-26T22:44:03.832771Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "wmt_test_data_loader = gluon.data.DataLoader(\n",
    "    wmt_data_test_with_len,\n",
    "    batch_sampler=wmt_test_batch_sampler,\n",
    "    batchify_fn=wmt_test_batchify_fn,\n",
    "    num_workers=8)  # Note that we can use multi-processing\n",
    "print('Number of testing batches:', len(wmt_test_data_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Evaluate Transformer\n",
    "\n",
    "Next, we evaluate the performance of the model on the `newstest2014` dataset. We first define the `BeamSearchTranslator` to generate the translations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T22:44:04.246173Z",
     "start_time": "2019-07-26T22:44:04.239136Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print('Beam Size =', hparams.beam_size, ', Lengh penalty Alpha=', hparams.lp_alpha, ', Length penalty K=', hparams.lp_k)\n",
    "wmt_translator = nmt.translation.BeamSearchTranslator(\n",
    "    model=wmt_transformer_model,\n",
    "    beam_size=hparams.beam_size,\n",
    "    scorer=nlp.model.BeamSearchScorer(alpha=hparams.lp_alpha, K=hparams.lp_k),\n",
    "    max_length=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Then we caculate the `loss` as well as the `bleu` score on the newstest2014 WMT 2014 English-German test dataset. This may take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T22:44:13.797203Z",
     "start_time": "2019-07-26T22:44:04.247901Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import utils\n",
    "\n",
    "eval_start_time = time.time()\n",
    "wmt_test_loss_function = nlp.loss.MaskedSoftmaxCELoss()\n",
    "wmt_test_loss_function.hybridize()\n",
    "wmt_detokenizer = nlp.data.SacreMosesDetokenizer()\n",
    "wmt_test_loss, wmt_test_translation_out = utils.evaluate(wmt_transformer_model,\n",
    "                                                         wmt_test_data_loader,\n",
    "                                                         wmt_test_loss_function,\n",
    "                                                         wmt_translator,\n",
    "                                                         wmt_tgt_vocab,\n",
    "                                                         wmt_detokenizer,\n",
    "                                                         ctx)\n",
    "wmt_test_bleu_score, _, _, _, _ = nmt.bleu.compute_bleu([wmt_test_tgt_sentences],\n",
    "                                                        wmt_test_translation_out,\n",
    "                                                        tokenized=False,\n",
    "                                                        tokenizer=hparams.bleu,\n",
    "                                                        split_compound_word=False,\n",
    "                                                        bpe=False)\n",
    "print('WMT14 EN-DE SOTA model test loss: %.2f; test bleu score: %.2f; time cost %.2fs' %(wmt_test_loss, wmt_test_bleu_score * 100, (time.time() - eval_start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T22:44:13.807062Z",
     "start_time": "2019-07-26T22:44:13.798912Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "print('Sample translations:')\n",
    "num_pairs = 1\n",
    "\n",
    "for i in range(num_pairs):\n",
    "    print('EN:')\n",
    "    print(wmt_test_text[i][0])\n",
    "    print('DE-Candidate:')\n",
    "    print(wmt_test_translation_out[i])\n",
    "    print('DE-Reference:')\n",
    "    print(wmt_test_tgt_sentences[i])\n",
    "    print('========')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Translation Inference\n",
    "\n",
    "We herein show the actual translation example (EN-DE) when given a source language using the SOTA Transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T22:44:13.968146Z",
     "start_time": "2019-07-26T22:44:13.808510Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "print('Translate the following English sentence into German:')\n",
    "\n",
    "sample_src_seq = 'We love language .'\n",
    "# sample_src_seq = 'We love language.'  # Try this too\n",
    "print('[\\'' + sample_src_seq + '\\']')\n",
    "sample_tgt_seq = utils.translate(wmt_translator, sample_src_seq, wmt_src_vocab, wmt_tgt_vocab, wmt_detokenizer,\n",
    "                                 ctx)\n",
    "print('The German translation is:')\n",
    "print(sample_tgt_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If you'd like to train your own transformer models, you may find the training scripts in our\n",
    "[scripts](https://github.com/dmlc/gluon-nlp/tree/master/scripts/machine_translation).\n",
    "\n",
    "## References\n",
    "\n",
    "[1] Vaswani, Ashish, et al. \"Attention is all you need.\" Advances in Neural Information Processing Systems. 2017."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
